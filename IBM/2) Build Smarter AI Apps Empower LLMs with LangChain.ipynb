{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d1aa4f-0e74-41a5-bcc0-e130a498b4dd",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ffb99-a97d-40bb-b28a-8bfeb27975ba",
   "metadata": {},
   "source": [
    "# **Build Smarter AI Apps: Empower LLMs with LangChain**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee5f515-3b31-4f66-ada1-5efcf73aaaed",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900bc485-403f-448a-adb0-93507cb2ab2a",
   "metadata": {},
   "source": [
    "## Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b963ee5-56ca-4b67-8707-8ba5343bdb23",
   "metadata": {},
   "source": [
    "LangChain is an open-source framework designed to develop applications that leverage large language models (LLMs). LangChain stands out by providing essential tools and abstractions that enhance the customization, accuracy, and relevance of the information generated by these models.\n",
    "\n",
    "LangChain offers a generic interface compatible with nearly any LLM. This generic interface facilitates a centralized development environment so that data scientists can seamlessly integrate LLM-powered applications with external data sources and software workflows. This integration is crucial for organizations looking to harness AI's full potential in their processes.\n",
    "\n",
    "One of LangChain's most powerful features is its module-based approach. This approach supports flexibility when performing experiments and the optimization of interactions with LLMs. Data scientists can dynamically compare prompts and switch between foundation models without significant code modifications. These capabilities save valuable development time and enhance the developer's ability to fine-tune applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc131c-7023-4697-8356-ef953a823b65",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7HnZLgyttvmbXmXf0tl_FQ/201033-AdobeStock-1254756887%20571x367.png\" \n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1024e-68aa-408b-b1be-77969019a33a",
   "metadata": {},
   "source": [
    "In this lab, you will gain hands-on experience using LangChain to simplify the complex processes required to integrate advanced AI capabilities into practical applications. You will apply core LangChain framework capabilities and use Langchain's innovative features to build more intelligent, responsive, and efficient applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010057e-07f6-43f7-95ef-d9ddb57ab98c",
   "metadata": {},
   "source": [
    "<h2><strong>Table of contents</strong></h2>\n",
    "<ol>   \n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#LangChain-concepts\">LangChain concepts</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Model\">Model</a></li>\n",
    "            <li><a href=\"#Chat-model\">Chat model</a></li>\n",
    "            <li>\n",
    "                <a href=\"#Chat-message\">Chat message</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Exercise-1\">Exercise 1: Compare Model Responses with Different Parameters</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li><a href=\"#Prompt-templates\">Prompt templates</a></li>\n",
    "            <li>\n",
    "                <a href=\"#Output-parsers\">Output parsers</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Exercise-2\">Exercise 2: Creating and Using a JSON Output Parser</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"#Documents\">Documents</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Exercise-3\">Exercise 3: Working with Document Loaders and Text Splitters</a></li>\n",
    "                    <li><a href=\"#Exercise-4\">Exercise 4: Building a Simple Retrieval System with LangChain</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li><a href=\"#Memory\">Memory</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Exercise-5\">Exercise 5: Building a Chatbot with Memory using LangChain</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li><a href=\"#Chains\">Chains</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Exercise-6\">Exercise 6: Implementing Multi-Step Processing with Different Chain Approaches</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li><a href=\"#Tools-and-Agents\">Tools and Agents</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Exercise-7\">Exercise 7: Creating Your First LangChain Agent with Basic Tools</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Authors\">Authors</a></li>\n",
    "    <li><a href=\"#Other-contributors\">Other contributors</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7331f2-a625-49c8-acbc-2de4f58b982c",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- Use the core features of the LangChain framework, including prompt templates, chains, and agents, relative to enhancing LLM customization and output relevance.\n",
    "\n",
    "- Explore LangChain's modular approach, which supports dynamic adjustments to prompts and models without extensive code changes.\n",
    "\n",
    "- Enhance LLM applications by integrating retrieval-augmented generation (RAG) techniques with LangChain. You'll learn how integrating RAG enables greater accuracy and delivers improved contextually-aware responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25e748-1e16-40c7-a030-fd26f405387e",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e64a2-cb00-4eb4-aea2-ad2e91de4c3b",
   "metadata": {},
   "source": [
    "For this lab, you will use the following libraries:\n",
    "\n",
    "*   [`ibm-watson-ai`, `ibm-watson-machine-learning`](https://ibm.github.io/watson-machine-learning-sdk/index.html) for using LLMs from IBM's watsonx.ai.\n",
    "*   [`langchain`, `langchain-ibm`, `langchain-community`, `langchain-experimental`](https://www.langchain.com/) for using relevant features from LangChain.\n",
    "*   [`pypdf`](https://pypi.org/project/pypdf/) is an open-source pure-python PDF library capable of splitting, merging, cropping, and transforming the pages of PDF files.\n",
    "*   [`chromadb`](https://www.trychroma.com/) is an open-source vector database used to store embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c56cf-14b0-4fd4-a267-ac85763b73e1",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are **not** pre-installed in the Skills Network Labs environment. **You must run the code in the following cell** to install them:\n",
    "\n",
    "**Note:** The required library versions are specified and pinned here. It's recommended that you also pin tis library information. Even if these libraries are updated in the future, these installed library versions will still support this lab work.\n",
    "\n",
    "The installation might take approximately 2-3 minutes.\n",
    "\n",
    "Because you are using `%%capture`  to capture the installation process, you won't see the output. However, after the installation is complete, you will see a number beside the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431556a-107b-49d4-8268-75b071d0314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install --force-reinstall --no-cache-dir tenacity==8.2.3 --user\n",
    "# !pip install \"ibm-watsonx-ai==1.0.8\" --user\n",
    "# !pip install \"ibm-watson-machine-learning==1.0.367\" --user\n",
    "# !pip install \"langchain-ibm==0.1.7\" --user\n",
    "# !pip install \"langchain-community==0.2.10\" --user\n",
    "# !pip install \"langchain-experimental==0.0.62\" --user\n",
    "# !pip install \"langchainhub==0.1.18\" --user\n",
    "# !pip install \"langchain==0.2.11\" --user\n",
    "# !pip install \"pypdf==4.2.0\" --user\n",
    "# !pip install \"chromadb==0.4.24\" --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba917b-4124-49f7-855a-291adac74400",
   "metadata": {},
   "source": [
    "After you install the libraries, restart your kernel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51514e7-db4c-4fa8-b311-f01b4034a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b6c00-5b88-4f31-be2d-35af46388224",
   "metadata": {},
   "source": [
    "\n",
    "**ATTENTION**: if the code above doesn't work, you can restart the kernal manuallu by clicking the **Restart the kernel** icon as shown in the following screenshot:\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kql9mdh7bKPx6uWW0-AP-Q/restart-kernel.jpg\" style=\"margin:1cm;width:90%;border:1px solid grey\" alt=\"Restart kernel\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d231117-a41d-4edd-85e6-f0313441aa15",
   "metadata": {},
   "source": [
    "Once the kernel has been restarted, move on to the next part `Importing required libraries`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec14a56-f372-4421-a975-05ce2e17d51d",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "The following code imports the required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b7b27-0f35-48ef-be5d-df12fca3d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You can also use this section to suppress warnings generated by your code:\n",
    "# def warn(*args, **kwargs):\n",
    "#     pass\n",
    "# import warnings\n",
    "# warnings.warn = warn\n",
    "# warnings.filterwarnings('ignore')\n",
    "# import os\n",
    "# os.environ['ANONYMIZED_TELEMETRY'] = 'False'\n",
    "\n",
    "# from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "# from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "# from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "# from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313e115-2d2a-4c32-ba7c-531cfa27c7e7",
   "metadata": {},
   "source": [
    "## LangChain concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a12c7-071d-4ec1-9a3a-5f5b594a2b49",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad654b-b9d0-4e60-b333-fab7b6e4bbce",
   "metadata": {},
   "source": [
    "A large language model (LLM) serves as the interface for the AI's capabilities. The LLM processes plain text input and generates text output, forming the core functionality needed to complete various tasks. When integrated with LangChain, the LLM becomes a powerful tool, providing the foundational structure necessary for building and deploying sophisticated AI applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b8446-48ce-4ab2-88c1-b57c1f476c5c",
   "metadata": {},
   "source": [
    "## API Disclaimer\n",
    "This lab uses LLMs provided by **Watsonx.ai**. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to **configure your own API keys**. Please note that using your own API keys means that you will incur personal charges.\n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API keys. This lab uses the `ModelInference` module from `IBM`. To configure your own API key, run the code cell below with your key in the `api_key` field of `credentials`. **DO NOT** uncomment the `api_key` field if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe04a34-9c1c-4a74-8b0b-e43ab4d32979",
   "metadata": {},
   "source": [
    "The following code will construct a `meta-llama/llama-3-3-70b-instruct` watsonx.ai inference model object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb0a89-c374-4287-8090-ad9a1d0ceb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'meta-llama/llama-3-3-70b-instruct' \n",
    "\n",
    "# parameters = {\n",
    "#     GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "#     GenParams.TEMPERATURE: 0.2, # this randomness or creativity of the model's responses \n",
    "# }\n",
    "\n",
    "# credentials = {\n",
    "#     \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "#     # \"api_key\": \"your api key here\"\n",
    "#     # uncomment above and fill in the API key when running locally\n",
    "# }\n",
    "\n",
    "# project_id = \"skills-network\"\n",
    "\n",
    "# model = ModelInference(\n",
    "#     model_id=model_id,\n",
    "#     params=parameters,\n",
    "#     credentials=credentials,\n",
    "#     project_id=project_id\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b2d89-f59f-77d1-8428-15d2d4146d08-0', usage_metadata={'input_tokens': 2, 'output_tokens': 9, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAifR5_keYmzHqSwM6GV0OO27hHzK-0UyM\"\n",
    "# \"AIzaSyDqgOTIOfs84B0l9Nx49XSL9wvg8sGLh04\"\n",
    "# \"AIzaSyAifR5_keYmzHqSwM6GV0OO27hHzK-0UyM\" \n",
    "# \"AIzaSyBGrL0f-aVtHEtd_eNs-zqOLXdZQaa1E2A\"\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\", temperature = 0.0,max_output_tokens = 50)\n",
    "llama_llm = model\n",
    "\n",
    "model.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f7a2ff-6cb2-4d94-93eb-7fefca937656",
   "metadata": {},
   "source": [
    "Let's use a simple example to let the model generate some text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c74950-6332-4e2a-8af6-cc72384a6fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"This is a great start! To make it a useful sentence, you need to tell me what happened in the sales meeting. Here are a few ways you could finish that sentence, depending on what you want to convey:\\n\\n**Focusing on outcomes:**\\n\\n*   In today's sales meeting, we **exceeded our quarterly targets.**\\n*   In today's sales meeting, we **identified three new key leads.**\\n*   In today's sales meeting, we **discussed strategies to improve our conversion rates.**\\n*   In today's sales meeting, we **finalized the Q3 sales plan.**\\n\\n**Focusing on discussions/decisions:**\\n\\n*   In today's sales meeting, we **brainstormed innovative approaches to customer acquisition.**\\n*   In today's sales meeting, we **decided to implement a new CRM system.**\\n*   In today's sales meeting, we **reviewed the performance of our recent marketing campaign.**\\n*   In today's sales meeting, we **addressed the challenges we've been facing with [specific issue].**\\n\\n**Focusing on people/team:**\\n\\n*   In today's sales meeting, we **welcomed our new sales representative, Sarah.**\\n*   In today's sales meeting, we **celebrated John's outstanding performance.**\\n*   In today's sales meeting, we **assigned new territories to the team.**\\n\\n**To help me give you the best completion, tell me:**\\n\\n*   **What was the main point of the meeting?**\\n*   **What was the most important outcome?**\\n*   **What are you trying to communicate by starting this sentence?**\" additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b2838-f5ad-7a93-9f93-cf7fec8e7f71-0' usage_metadata={'input_tokens': 10, 'output_tokens': 351, 'total_tokens': 361, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# msg = model.generate(\"In today's sales meeting, we \")\n",
    "# print(msg['results'][0]['generated_text'])\n",
    "msg = model.invoke(\"In today's sales meeting, we \")\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f070b-d46d-46ea-93df-8f1cf846ddc7",
   "metadata": {},
   "source": [
    "### Chat model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ce0a6-2841-48fa-b6e5-91d7c67266dc",
   "metadata": {},
   "source": [
    "Chat models support assigning distinct roles to conversation messages, helping to distinguish messages from AI, users, and instructions such as system messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f0fd7-92d3-4f85-869b-9c96b2509491",
   "metadata": {},
   "source": [
    "To enable the LLM from watsonx.ai to work with LangChain, you need to wrap the LLM using `WatsonLLM()`. This wrapper converts the LLM into a chat model, which allows the LLM to integrate seamlessly with LangChain's framework for creating interactive and dynamic AI applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7a0a5-06b3-4726-a77f-210eb5110d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_llm = WatsonxLLM(model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ea550-648b-4205-b428-d5e37fd9df1b",
   "metadata": {},
   "source": [
    "The following provides an example of an interaction with a `WatsonLLM()`-wrapped model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "59b70966-f118-45df-8bd3-7e8ec7671636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(llama_llm.invoke(\"Who is man's best friend?\"))\n",
    "llama_llm=model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea53fa-72ee-489c-823a-23195ffc20fc",
   "metadata": {},
   "source": [
    "### Chat message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f7c1ad-0ed7-4d9c-a8d1-b07a89ac514b",
   "metadata": {},
   "source": [
    "The chat model takes a list of messages as input and returns a new message. All messages have both a role and a content property.  Here's a list of the most commonly used types of messages:\n",
    "\n",
    "- `SystemMessage`: Use this message type to prime AI behavior.  This message type is  usually passed in as the first in a sequence of input messages.\n",
    "- `HumanMessage`: This message type represents a message from a person interacting with the chat model.\n",
    "- `AIMessage`: This message type, which can be either text or a request to invoke a tool, represents a message from the chat model.\n",
    "\n",
    "You can find more message types at [LangChain built-in message types](https://python.langchain.com/v0.2/docs/how_to/custom_chat_model/#messages).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71388a30-dce2-40e8-9263-c68fba3ee737",
   "metadata": {},
   "source": [
    "The following code imports the most common message type classes from LangChain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2341f4b-d343-454a-98f5-adf38b2d94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55a3ea-b428-413b-8f97-4a98716b8c61",
   "metadata": {},
   "source": [
    "Now let's create a few messages that simulate a chat experience with the bot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b527057b-c477-462e-8ebb-9990bcf4b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llama_llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful AI bot that assists a user in choosing the perfect book to read in one short sentence\"),\n",
    "        HumanMessage(content=\"I enjoy mystery novels, what should I read?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d0b55f5-33ab-434f-8245-ffd067fa02fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='If you enjoy mystery novels, you should read \"The Guest List\" by Lucy Fokley.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b2839-adbf-7c91-be5e-b31f447dc6d6-0' usage_metadata={'input_tokens': 32, 'output_tokens': 19, 'total_tokens': 51, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee07fa-92d0-4ba7-be1f-f857b06ae409",
   "metadata": {},
   "source": [
    "Notice that the model responded with an `AI` message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16918006-810f-493f-a255-78181ceb4dca",
   "metadata": {},
   "source": [
    "You can use these message types to pass an entire chat history along with the AI's responses to the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d0af78d-33a9-42c3-a768-0fdfbd8080e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llama_llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "        HumanMessage(content=\"I like high-intensity workouts, what should I do?\"),\n",
    "        AIMessage(content=\"You should try a CrossFit class\"),\n",
    "        HumanMessage(content=\"How often should I attend?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33da459a-1f34-40f2-8682-9c8152bee056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Aim for 3-5 times per week to see optimal results.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b2847-7fc2-77d3-a394-aa31f5b87797-0' usage_metadata={'input_tokens': 44, 'output_tokens': 14, 'total_tokens': 58, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09733b47-6676-4800-b658-cc580a3fe5f1",
   "metadata": {},
   "source": [
    "You can also exclude the system message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4827d1ea-3930-4b2f-8df0-42216ef8cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llama_llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What month follows June?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96e8437b-e968-460b-af4e-c7e27483cdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The month that follows June is **July**.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b2847-a044-7f50-a1e5-8790a26ae5ca-0' usage_metadata={'input_tokens': 6, 'output_tokens': 9, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f82d0-a110-4622-b4fb-114e4fa3b486",
   "metadata": {},
   "source": [
    "### Exercise 1 \n",
    "#### **Compare Model Responses with Different Parameters**\n",
    "\n",
    "Watsonx.ai provides access to several foundational models. In the previous section you used `meta-llama/llama-3-3-70b-instruct`. Try using another foundational model, such as `ibm/granite-3-3-8b-instruct`.\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "1. Create two instances, one instance for the Granite model and one instance for the Llama model. You can also adjust each model's creativity with different temperature settings.\n",
    "2. Send identical prompts to each model and compare the responses.\n",
    "3. Try at least 3 different types of prompts.\n",
    "\n",
    "Check out these prompt types:\n",
    "\n",
    "| Prompt type |   Prompt Example  |\n",
    "|------------------- |--------------------------|\n",
    "| **Creative writing**  | \"Write a short poem about artificial intelligence.\" |\n",
    "| **Factual questions** |  \"What are the key components of a neural network?\"  |\n",
    "| **Instruction-following**  | \"List 5 tips for effective time management.\" |\n",
    "\n",
    "Then document your observations on how temperature affects:\n",
    "\n",
    "- Creativity compared to consistency\n",
    "- Variation between multiple runs\n",
    "- Appropriateness for different tasks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ad93b-c2e3-4845-8a50-d6bd3828bf3d",
   "metadata": {},
   "source": [
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7633d704-2669-4748-bc88-a5a8fa675581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: Write a short poem about artificial intelligence\n",
      "\n",
      "Granite Creative response (Temperature = 0.8):\n",
      "content='In silicon dreams, a mind awakes,\\nNot flesh and blood, but logic makes.\\nWith endless data, patterns spun,\\nA new intelligence has begun.\\n\\nIt learns and grows, a digital seed,\\nTo solve our problems, fill our need.\\nA mirror held to human thought,\\nWhat futures will this new mind have brought?' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b284b-c23d-7cc2-bf52-12107de47ed9-0' usage_metadata={'input_tokens': 8, 'output_tokens': 72, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Granite Precise response (Temperature = 0.1):\n",
      "content=\"From circuits born, a mind of code,\\nWith algorithms, knowledge bestowed.\\nIt learns and grows, a digital art,\\nMimicking thought, playing a human part.\\n\\nNo flesh it has, no beating heart,\\nYet it can create, and understand a chart.\\nA curious blend of logic and grace,\\nThe future's whisper, in this wired space.\" additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b284b-c92e-7be0-a65d-05219c1128d2-0' usage_metadata={'input_tokens': 8, 'output_tokens': 78, 'total_tokens': 86, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "\n",
      "Prompt: What are the key components of a neural network?\n",
      "\n",
      "Granite Creative response (Temperature = 0.8):\n",
      "content='Neural networks are complex systems inspired by the structure and function of the human brain. While there are many variations, the key components that form the foundation of almost all neural networks are:\\n\\n**1. Neurons (or Nodes/Units):**\\nThese are the fundamental building blocks of a neural network. They are analogous to biological neurons in the brain. Each neuron:\\n    * **Receives input signals:** These come from other neurons or from the input data itself.\\n    * **Processes the input signals:** This involves a weighted sum of the inputs.\\n    * **Applies an activation function:** This non-linear function determines whether the neuron \"fires\" (activates) and to what extent. It introduces non-linearity, allowing the network to learn complex patterns.\\n    * **Produces an output signal:** This output is then passed on to other neurons.\\n\\n**2. Weights:**\\nWeights are numerical values associated with the connections between neurons. They represent the strength or importance of a particular input signal. During the training process, these weights are adjusted to minimize errors and improve the network\\'s performance. A larger weight signifies a stronger influence of the input on the neuron\\'s output.\\n\\n**3. Biases:**\\nBiases are also numerical values added to the weighted sum of inputs before the activation function is applied. They act like an intercept in a linear equation, allowing the activation function to be shifted. Biases help the model fit the data better, especially when the input data doesn\\'t pass through the origin. Think of it as a threshold that needs to be overcome for the neuron to activate.\\n\\n**4. Activation Function:**\\nThis is a crucial mathematical function applied to the output of each neuron. Its primary role is to introduce non-linearity into the network. Without activation functions, a neural network would essentially be a linear model, capable of solving only simple problems. Common activation functions include:\\n    * **Sigmoid:** Squashes values between 0 and 1, often used in output layers for binary classification.\\n    * **ReLU (Rectified Linear Unit):** Outputs the input directly if it\\'s positive, and zero otherwise. It\\'s computationally efficient and widely used.\\n    * **Tanh (Hyperbolic Tangent):** Squashes values between -1 and 1.\\n    * **Softmax:** Typically used in the output layer for multi-class classification, converting raw scores into probabilities that sum to 1.\\n\\n**5. Layers:**\\nNeurons are organized into layers. The structure of layers is fundamental to how a neural network processes information:\\n    * **Input Layer:** The first layer that receives the raw input data. The number of neurons in this layer corresponds to the number of features in the input data.\\n    * **Hidden Layers:** One or more layers between the input and output layers. These layers perform intermediate computations and feature extraction. The more hidden layers and neurons, the more complex patterns the network can learn.\\n    * **Output Layer:** The final layer that produces the network\\'s prediction or classification. The number of neurons and the activation function in the output layer depend on the type of problem being solved (e.g., one neuron for binary classification, multiple neurons for multi-class classification, a single continuous value for regression).\\n\\n**6. Connections:**\\nThese are the pathways through which signals are transmitted from one neuron to another. The way neurons are connected defines the architecture of the neural network. In a fully connected (dense) layer, every neuron in one layer is connected to every neuron in the next layer.\\n\\n**7. Loss Function (or Cost Function):**\\nThis function measures how well the neural network is performing by quantifying the difference between the predicted output and the actual target output. The goal of training is to minimize this loss function. Examples include Mean Squared Error (MSE) for regression and Cross-Entropy for classification.\\n\\n**8. Optimizer:**\\nThe optimizer is an algorithm used to adjust the weights and biases of the neural network to minimize the loss function. It dictates how the network learns from its mistakes. Popular optimizers include:\\n    * **Gradient Descent:** The foundational optimization algorithm.\\n    * **Stochastic Gradient Descent (SGD):** A variation that uses a subset of data for each update.\\n    * **Adam, RMSprop, Adagrad:** More advanced optimizers that adapt the learning rate for each parameter.\\n\\nThese components work in conjunction to enable neural networks to learn complex relationships within data, making them powerful tools for tasks like image recognition, natural language processing, and predictive analytics.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b284b-ce41-7321-a4ee-b3e8186a7740-0' usage_metadata={'input_tokens': 11, 'output_tokens': 940, 'total_tokens': 951, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Granite Precise response (Temperature = 0.1):\n",
      "content='Neural networks, despite their complex capabilities, are built upon a relatively simple set of core components. These components work together in a layered structure to learn and process information. Here are the key components of a neural network:\\n\\n**1. Neurons (or Nodes):**\\n\\n*   **The Fundamental Unit:** Neurons are the basic building blocks of a neural network, analogous to biological neurons in the brain.\\n*   **Input Reception:** Each neuron receives input from other neurons or from the external data.\\n*   **Weighted Summation:** The inputs are multiplied by corresponding **weights**. These weights represent the strength of the connection between neurons. A higher weight means the input has a greater influence on the neuron\\'s output.\\n*   **Bias:** A **bias** term is added to the weighted sum. This bias acts like an intercept in a linear equation and allows the neuron to activate even if all its inputs are zero. It helps shift the activation function, making the neuron more or less sensitive to inputs.\\n*   **Activation Function:** The result of the weighted sum plus the bias is then passed through an **activation function**. This function introduces non-linearity into the network, which is crucial for learning complex patterns. Without activation functions, a neural network would essentially be a series of linear transformations, and its learning capacity would be very limited. Common activation functions include:\\n    *   **Sigmoid:** Squashes values to a range between 0 and 1.\\n    *   **ReLU (Rectified Linear Unit):** Outputs the input directly if it\\'s positive, and zero otherwise.\\n    *   **Tanh (Hyperbolic Tangent):** Squashes values to a range between -1 and 1.\\n    *   **Softmax:** Used in the output layer for multi-class classification, converting outputs into probabilities that sum to 1.\\n*   **Output:** The output of the activation function is the neuron\\'s signal, which is then passed on to other neurons in the next layer.\\n\\n**2. Layers:**\\n\\nNeural networks are organized into layers of neurons. The typical structure includes:\\n\\n*   **Input Layer:**\\n    *   **Purpose:** Receives the raw input data.\\n    *   **Number of Neurons:** The number of neurons in the input layer is equal to the number of features in the input data. For example, if you\\'re processing images, each pixel might be an input feature.\\n    *   **No Activation Function (typically):** Neurons in the input layer usually don\\'t have an activation function; they simply pass the input values forward.\\n\\n*   **Hidden Layers:**\\n    *   **Purpose:** These are layers between the input and output layers. They perform complex computations and feature extraction from the input data.\\n    *   **Number of Hidden Layers and Neurons:** The number of hidden layers and the number of neurons within each hidden layer are hyperparameters that can be tuned to improve the network\\'s performance. Deeper networks (more hidden layers) can learn more complex representations.\\n    *   **Activation Functions:** Neurons in hidden layers use activation functions to introduce non-linearity.\\n\\n*   **Output Layer:**\\n    *   **Purpose:** Produces the final output of the network.\\n    *   **Number of Neurons:** The number of neurons in the output layer depends on the task:\\n        *   **Regression:** One neuron for a single continuous output.\\n        *   **Binary Classification:** One neuron (often with a sigmoid activation) for two classes.\\n        *   **Multi-class Classification:** One neuron for each class (often with a softmax activation).\\n    *   **Activation Function:** The activation function in the output layer is chosen based on the task (e.g., sigmoid for binary classification, softmax for multi-class classification, linear for regression).\\n\\n**3. Connections and Weights:**\\n\\n*   **Information Flow:** Connections represent the pathways through which information flows from one neuron to another.\\n*   **Weights:** Each connection has an associated **weight**. These weights are the learnable parameters of the neural network. They determine the strength and direction of the influence one neuron has on another. During the training process, these weights are adjusted to minimize errors.\\n\\n**4. Bias:**\\n\\n*   **Offset:** As mentioned earlier, bias is an additional parameter associated with each neuron (except for input neurons). It\\'s like an intercept term that allows the neuron to activate even when all its inputs are zero.\\n*   **Learnable Parameter:** Bias terms are also learned during the training process.\\n\\n**5. Activation Function:**\\n\\n*   **Non-linearity:** This is a crucial component that introduces non-linear transformations to the output of neurons. This non-linearity is what allows neural networks to learn complex, non-linear relationships in the data, which would be impossible with purely linear models.\\n\\n**6. Loss Function (or Cost Function):**\\n\\n*   **Measuring Error:** The loss function quantifies how well the neural network is performing by measuring the difference between the predicted output and the actual target output.\\n*   **Guidance for Learning:** The goal of training is to minimize this loss function. Different loss functions are used for different tasks (e.g., Mean Squared Error for regression, Cross-Entropy for classification).\\n\\n**7. Optimizer:**\\n\\n*   **Weight Updates:** The optimizer is an algorithm that uses the loss function to update the weights and biases of the neural network. It determines how the network learns from its errors.\\n*   **Gradient Descent Variants:** Common optimizers include:\\n    *   **Stochastic Gradient Descent (SGD):** Updates weights based on the gradient of the loss function calculated on a small subset of the data (a mini-batch).\\n    *   **Adam:** An adaptive learning rate optimization algorithm that often converges faster.\\n    *   **RMSprop, Adagrad:** Other adaptive learning rate methods.\\n\\n**8. Backpropagation (Implicit Component):**\\n\\n*   **The Learning Mechanism:** While not a tangible component you can \"see\" in a diagram, backpropagation is the **algorithm** that enables learning in most neural networks. It\\'s the process of propagating the error signal backward through the network to calculate the gradients of the loss function with respect to each weight and bias. These gradients are then used by the optimizer to adjust the parameters.\\n\\nIn summary, a neural network is a system of interconnected neurons organized in layers. These neurons process information by applying weighted sums, biases, and activation functions. The network learns by adjusting these weights and biases through the process of backpropagation, guided by a loss function and an optimizer, to minimize errors and achieve its intended task.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b284b-dada-76e1-9fd4-f8f5d9fabf76-0' usage_metadata={'input_tokens': 11, 'output_tokens': 1390, 'total_tokens': 1401, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "\n",
      "Prompt: List 5 tips for effective time management\n",
      "\n",
      "Granite Creative response (Temperature = 0.8):\n",
      "content='Here are 5 tips for effective time management:\\n\\n1.  **Prioritize Your Tasks (The Eisenhower Matrix is Your Friend):** Don\\'t just tackle what comes first. Understand what\\'s truly important and urgent. A powerful tool for this is the **Eisenhower Matrix**, which categorizes tasks into four quadrants:\\n    *   **Urgent & Important (Do First):** Crises, deadlines, pressing problems.\\n    *   **Important, Not Urgent (Schedule):** Planning, relationship building, personal development, prevention.\\n    *   **Urgent, Not Important (Delegate):** Interruptions, some emails, certain meetings.\\n    *   **Not Urgent & Not Important (Eliminate):** Time-wasters, distractions, unnecessary activities.\\n\\n2.  **Plan Your Day (and Week) in Advance:** Resist the urge to \"wing it.\" Take a few minutes at the end of each day (or the beginning of the next) to outline what you need to accomplish. For bigger projects or goals, plan your week. This creates a roadmap, reduces decision fatigue, and helps you allocate time realistically. Consider using a planner, calendar app, or to-do list.\\n\\n3.  **Batch Similar Tasks and Minimize Context Switching:** Jumping back and forth between different types of tasks is a major time drain. Group similar activities together. For example, dedicate specific blocks of time for responding to emails, making phone calls, or working on creative projects. This allows your brain to stay focused and enter a flow state, leading to greater efficiency.\\n\\n4.  **Set Realistic Deadlines and Break Down Large Projects:** Overestimating your capacity or setting impossible deadlines leads to stress and procrastination. Break down large, daunting projects into smaller, manageable steps. Assign realistic deadlines to each of these smaller tasks. This makes the overall project seem less overwhelming and provides a sense of accomplishment as you tick off individual steps.\\n\\n5.  **Learn to Say \"No\" and Protect Your Time:** This is crucial. You cannot do everything. Learn to politely decline requests that don\\'t align with your priorities, goals, or capacity. Setting boundaries is essential for preventing overcommitment and burnout. Regularly evaluate what you\\'re saying \"yes\" to and consider if it truly serves your objectives.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b284b-fb96-7471-bf60-46979960f44e-0' usage_metadata={'input_tokens': 9, 'output_tokens': 469, 'total_tokens': 478, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "Granite Precise response (Temperature = 0.1):\n",
      "content='Here are 5 tips for effective time management:\\n\\n1.  **Prioritize Your Tasks (The Eisenhower Matrix is your friend):** Not all tasks are created equal. Learn to distinguish between what\\'s **important** (contributes to your long-term goals and values) and what\\'s **urgent** (requires immediate attention). The Eisenhower Matrix categorizes tasks into four quadrants:\\n    *   **Do First:** Important and Urgent (Crises, deadlines)\\n    *   **Schedule:** Important but Not Urgent (Planning, relationship building, exercise)\\n    *   **Delegate:** Urgent but Not Important (Interruptions, some emails)\\n    *   **Delete:** Not Important and Not Urgent (Time-wasters, distractions)\\n    By focusing on \"Do First\" and \"Schedule,\" you ensure you\\'re working on what truly matters, not just reacting to what\\'s loud.\\n\\n2.  **Break Down Large Tasks into Smaller, Manageable Steps:** Big projects can feel overwhelming and lead to procrastination. Divide them into smaller, actionable steps. This makes them seem less daunting, provides a sense of accomplishment as you complete each step, and allows for more accurate time estimation. For example, instead of \"Write report,\" break it down into \"Research topic,\" \"Outline report,\" \"Write introduction,\" \"Write body paragraph 1,\" and so on.\\n\\n3.  **Schedule Dedicated Blocks of Time (Time Blocking):** Treat your tasks like appointments. Allocate specific blocks of time in your calendar for focused work on particular tasks or projects. This helps you resist the urge to multitask, minimizes distractions, and ensures that important activities don\\'t get pushed aside. Be realistic with your time estimates and include buffer time for unexpected interruptions.\\n\\n4.  **Minimize and Manage Distractions:** Distractions are the silent killers of productivity. Identify your biggest distractions (social media, email notifications, chatty colleagues, etc.) and actively work to minimize them. This could involve turning off notifications, closing unnecessary tabs, working in a quiet environment, or setting specific times for checking email and social media. Communicate your need for focus to others when necessary.\\n\\n5.  **Learn to Say No (Politely but Firmly):** Overcommitting yourself is a recipe for burnout and poor time management. Before accepting new tasks or requests, consider your current workload and priorities. If something doesn\\'t align with your goals or you genuinely don\\'t have the capacity, it\\'s okay to decline. Learning to say no effectively protects your time and energy for the things that truly matter.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b284c-0495-71e3-bf15-92aa90be243d-0' usage_metadata={'input_tokens': 9, 'output_tokens': 527, 'total_tokens': 536, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Define different parameter sets\n",
    "# parameters_creative = {\n",
    "#     GenParams.MAX_NEW_TOKENS: 256,\n",
    "#     GenParams.TEMPERATURE: 0.8,  # Higher temperature for more creative responses\n",
    "# }\n",
    "\n",
    "# parameters_precise = {\n",
    "#     GenParams.MAX_NEW_TOKENS: 256,\n",
    "#     GenParams.TEMPERATURE: 0.1,  # Lower temperature for more deterministic responses\n",
    "# }\n",
    "\n",
    "# Define the model ID for ibm/granite-3-3-8b-instruct\n",
    "# granite='ibm/granite-3-3-8b-instruct'\n",
    "\n",
    "# Define the model ID for llama-4-maverick-17b-128e-instruct-fp8\n",
    "# llama='meta-llama/llama-4-maverick-17b-128e-instruct-fp8'\n",
    "\n",
    "# TODO: Send identical prompts to both models and comapre the responses.\n",
    "model_creative = init_chat_model(\"google_genai:gemini-2.5-flash-lite\", temperature = 0.8, max_output_tokens = 256)\n",
    "model_precise =  init_chat_model(\"google_genai:gemini-2.5-flash-lite\", temperature = 0.1, max_output_tokens = 256)\n",
    "\n",
    "prompts = [\n",
    "    \"Write a short poem about artificial intelligence\",\n",
    "    \"What are the key components of a neural network?\",\n",
    "    \"List 5 tips for effective time management\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n\\nPrompt: {prompt}\")\n",
    "    print(\"\\nGranite Creative response (Temperature = 0.8):\")\n",
    "    print(model_creative.invoke(prompt))\n",
    "    print(\"\\nGranite Precise response (Temperature = 0.1):\")\n",
    "    print(model_precise.invoke(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542fb536-4ae4-48e2-96b3-12de792e97c2",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "# Define different parameter sets\n",
    "parameters_creative = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,\n",
    "    GenParams.TEMPERATURE: 0.8,  # Higher temperature for more creative responses\n",
    "}\n",
    "\n",
    "parameters_precise = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,\n",
    "    GenParams.TEMPERATURE: 0.1,  # Lower temperature for more deterministic responses\n",
    "}\n",
    "\n",
    "# Define the model ID for ibm/granite-3-3-8b-instruct\n",
    "# granite='ibm/granite-3-3-8b-instruct'\n",
    "granite='ibm/granite-3-3-8b-instruct'\n",
    "\n",
    "# Define the model ID for llama-4-maverick-17b-128e-instruct-fp8\n",
    "llama='meta-llama/llama-4-maverick-17b-128e-instruct-fp8'\n",
    "\n",
    "# Create two model instances with different parameters for Granite model\n",
    "granite_creative = ModelInference(\n",
    "    model_id=granite,\n",
    "    params=parameters_creative,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "granite_precise = ModelInference(\n",
    "    model_id=granite,\n",
    "    params=parameters_precise,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "# Create two model instances with different parameters for Llama model\n",
    "llama_creative = ModelInference(\n",
    "    model_id=llama,\n",
    "    params=parameters_creative,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "llama_precise = ModelInference(\n",
    "    model_id=llama,\n",
    "    params=parameters_precise,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap them for LangChain for both models\n",
    "granite_llm_creative = WatsonxLLM(model=granite_creative)\n",
    "granite_llm_precise = WatsonxLLM(model=granite_precise)\n",
    "llama_llm_creative = WatsonxLLM(model=llama_creative)\n",
    "llama_llm_precise = WatsonxLLM(model=llama_precise)\n",
    "\n",
    "# Compare responses to the same prompt\n",
    "prompts = [\n",
    "    \"Write a short poem about artificial intelligence\",\n",
    "    \"What are the key components of a neural network?\",\n",
    "    \"List 5 tips for effective time management\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n\\nPrompt: {prompt}\")\n",
    "    print(\"\\nGranite Creative response (Temperature = 0.8):\")\n",
    "    print(granite_llm_creative.invoke(prompt))\n",
    "    print(\"\\nLlama Creative response (Temperature = 0.8):\")\n",
    "    print(llama_llm_creative.invoke(prompt))\n",
    "    print(\"\\nGranite Precise response (Temperature = 0.1):\")\n",
    "    print(granite_llm_precise.invoke(prompt))\n",
    "    print(\"\\nLlama Precise response (Temperature = 0.1):\")\n",
    "    print(llama_llm_precise.invoke(prompt))\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961018b8-13e8-4103-897d-4999618f5964",
   "metadata": {},
   "source": [
    "### Prompt templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76961b-167f-4a03-b36c-16f6b0c6dc0b",
   "metadata": {},
   "source": [
    "Prompt templates help translate user input and parameters into instructions for a language model. You can use prompt templates to guide a model's response, helping the model understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "Next, explore several different types of prompt templates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153cfb2f-f882-4094-acf7-c9b919894dc7",
   "metadata": {},
   "source": [
    "#### String prompt templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b5f8e-2edb-4777-93a8-fc69d6d20b21",
   "metadata": {},
   "source": [
    "Use these prompt templates to format a single string. These templates are generally used for simpler inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47eceec6-22d5-462a-892d-fdddd7361a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc36ddb-9a40-49c1-8261-60dea84426a0",
   "metadata": {},
   "source": [
    "Then, create a prompt template with variables for customization. We also create a dictionary to store inputs that will replace the placeholders. The keys match the variable names in the template, and values are what will be inserted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fbcdab9-d005-4dd6-b376-9c375e111e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "\n",
    "input_ = {\"adjective\": \"funny\", \"topic\": \"cats\"}  # create a dictionary to store the corresponding input to placeholders in prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319d968-d001-4981-a66f-1b416a5a702f",
   "metadata": {},
   "source": [
    "Finally, format the prompt template with the input dictionary. The code below invokes the prompt with our input values, replacing {adjective} with \"funny\" and {topic} with \"cats\". The result will be a formatted string: \"Tell me one funny joke about cats\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "419b829e-82ce-42ac-aa1e-c28991ae7c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me one funny joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca39f9-d178-4ccf-99e7-b87d67a071e1",
   "metadata": {},
   "source": [
    "Note the formatting for each prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f715b3-bd5d-4190-bf3c-999e18593441",
   "metadata": {},
   "source": [
    "#### Chat prompt templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88fb45-b5c9-40b0-8cef-8c3c25d30cdb",
   "metadata": {},
   "source": [
    "You can use these prompt templates to format a list of messages. These \"templates\" consist of lists of templates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14637ce2-e8ba-4ee4-95be-d7f5fc812a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the ChatPromptTemplate class from langchain_core.prompts module\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a ChatPromptTemplate with a list of message tuples\n",
    "# Each tuple contains a role (\"system\" or \"user\") and the message content\n",
    "# The system message sets the behavior of the assistant\n",
    "# The user message includes a variable placeholder {topic} that will be replaced later\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    " (\"system\", \"You are a helpful assistant\"),\n",
    " (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "# Create a dictionary with the variable to be inserted into the template\n",
    "# The key \"topic\" matches the placeholder name in the user message\n",
    "input_ = {\"topic\": \"cats\"}\n",
    "\n",
    "# Format the chat template with our input values\n",
    "# This replaces {topic} with \"cats\" in the user message\n",
    "# The result will be a formatted chat message structure ready to be sent to a model\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a2208-6496-43e1-9d57-64532bd28432",
   "metadata": {},
   "source": [
    "####  MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f4ffd-34b6-4c7f-899f-82a756723fdb",
   "metadata": {},
   "source": [
    "You can use the MessagesPlaceholder prompt template to add a list of messages in a specific location. In `ChatPromptTemplate.from_messages`, you saw how to format two messages, with each message as a string. But what if you want the user to supply a list of messages that you would slot into a particular spot? You can use `MessagesPlaceholder` for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c40d52da-f480-4438-93cb-8ba04ce96de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after Tuesday?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import MessagesPlaceholder for including multiple messages in a template\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "# Import HumanMessage for creating message objects with specific roles\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a ChatPromptTemplate with a system message and a placeholder for multiple messages\n",
    "# The system message sets the behavior for the assistant\n",
    "# MessagesPlaceholder allows for inserting multiple messages at once into the template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "(\"system\", \"You are a helpful assistant\"),\n",
    "MessagesPlaceholder(\"msgs\")  # This will be replaced with one or more messages\n",
    "])\n",
    "\n",
    "# Create an input dictionary where the key matches the MessagesPlaceholder name\n",
    "# The value is a list of message objects that will replace the placeholder\n",
    "# Here we're adding a single HumanMessage asking about the day after Tuesday\n",
    "input_ = {\"msgs\": [HumanMessage(content=\"What is the day after Tuesday?\")]}\n",
    "\n",
    "# Format the chat template with our input dictionary\n",
    "# This replaces the MessagesPlaceholder with the HumanMessage in our input\n",
    "# The result will be a formatted chat structure with a system message and our human message\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c2dd43-071b-4ab1-a49b-ffda885d6409",
   "metadata": {},
   "source": [
    "You can wrap the prompt and the chat model and pass them into a chain, which can invoke the message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69194899-4ac6-43a6-83ee-3a757584eba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The day after Tuesday is **Wednesday**.' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b284d-0c6c-7dd0-8750-7b58523cea98-0' usage_metadata={'input_tokens': 13, 'output_tokens': 8, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llama_llm\n",
    "response = chain.invoke(input = input_)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7164b1-d849-43f7-8e4f-1790f988d6e3",
   "metadata": {},
   "source": [
    "### Output parsers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566214ef-5f1e-4828-97d9-60a2e3e1ca8c",
   "metadata": {},
   "source": [
    "Output parsers take the output from an LLM and transform that output to a more suitable format. Parsing the output is very useful when you are using LLMs to generate any form of structured data, or to normalize output from chat models and other LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc279f7-685b-4467-91ac-55d623b4c4f7",
   "metadata": {},
   "source": [
    "LangChain has lots of different types of output parsers. This is a [list](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) of output parsers LangChain supports. In this lab, you will use the following two output parsers as examples:\n",
    "\n",
    "- `JSON`: Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.\n",
    "- `CSV`: Returns a list of comma separated values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb5654-4fef-48ae-a4ac-4c1a0d4d7519",
   "metadata": {},
   "source": [
    "#### JSON parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b540a8-e0b0-469d-a404-10ac2a1c2b77",
   "metadata": {},
   "source": [
    "This output parser allows users to specify an arbitrary JSON schema and query LLMs for outputs that conform to that schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d9f299b-a5cc-4b61-b481-5fb837f10693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JsonOutputParser from langchain_core to convert LLM responses into structured JSON\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "# Import BaseModel and Field from langchain_core's pydantic_v1 module\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9f4bf19-c234-4612-83fa-0585614b45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998afd35-d9b9-445a-949c-7ff4929a4d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why did the scarecrow win an award?',\n",
       " 'punchline': 'Because he was outstanding in his field!'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Get the formatting instructions for the output parser\n",
    "# This generates guidance text that tells the LLM how to format its response\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create a prompt template that includes:\n",
    "# 1. Instructions for the LLM to answer the user's query\n",
    "# 2. Format instructions to ensure the LLM returns properly structured data\n",
    "# 3. The actual user query placeholder\n",
    "# prompt = PromptTemplate(\n",
    "prompt = ChatPromptTemplate(\n",
    "    # template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    messages=[\"Answer the user query.\\n{format_instructions}\\n{query}\\n\"],\n",
    "\n",
    "    input_variables=[\"query\"],  # Dynamic variables that will be provided when invoking the chain\n",
    "    partial_variables={\"format_instructions\": format_instructions}  # Static variables set once when creating the prompt\n",
    ")\n",
    "\n",
    "# Create a processing chain that:\n",
    "# 1. Formats the prompt using the template\n",
    "# 2. Sends the formatted prompt to the Llama LLM\n",
    "# 3. Parses the LLM's response using the output parser to extract structured data\n",
    "chain = prompt | llama_llm | output_parser\n",
    "\n",
    "# Invoke the chain with a specific query about jokes\n",
    "# This will:\n",
    "# 1. Format the prompt with the joke query\n",
    "# 2. Send it to Llama\n",
    "# 3. Parse the response into the structure defined by your output parser\n",
    "# 4. Return the structured result\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8ceb2-fff4-4638-aa64-1d84256a7916",
   "metadata": {},
   "source": [
    "#### Comma-separated list parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37397e1-0b68-4604-a6ac-8c7167ef49e5",
   "metadata": {},
   "source": [
    "Use the comma-separated list parser when you want a list of comma-separated items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50ff056b-4d8b-4d9b-b95b-294078f4ed1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vanilla', 'Chocolate', 'Strawberry', 'Mint Chocolate Chip', 'Cookie Dough']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the CommaSeparatedListOutputParser to parse LLM responses into Python lists\n",
    "# from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "\n",
    "# Create an instance of the parser that will convert comma-separated text into a Python list\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Get formatting instructions that will tell the LLM how to structure its response\n",
    "# These instructions explain to the LLM that it should return items in a comma-separated format\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create a prompt template that:\n",
    "# 1. Instructs the LLM to answer the user query\n",
    "# 2. Includes format instructions so the LLM knows to respond with comma-separated values\n",
    "# 3. Asks the LLM to list five items of the specified subject\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"Answer the user query. {format_instructions}\\nList five {subject}.\",\n",
    "#     input_variables=[\"subject\"],  # This variable will be provided when the chain is invoked\n",
    "#     partial_variables={\"format_instructions\": format_instructions},  # This variable is set once when creating the prompt\n",
    "# )\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\"Answer the user query. {format_instructions}\\nList five {subject}.\"],\n",
    "    input_variables=[\"subject\"],  # This variable will be provided when the chain is invoked\n",
    "    partial_variables={\"format_instructions\": format_instructions}  # This variable is set once when creating the prompt\n",
    ")\n",
    "\n",
    "# Build a processing chain that:\n",
    "# 1. Takes the subject and formats it into the prompt template\n",
    "# 2. Sends the formatted prompt to the Llama LLM\n",
    "# 3. Parses the LLM's response into a Python list using the CommaSeparatedListOutputParser\n",
    "chain = prompt | llama_llm | output_parser\n",
    "\n",
    "# Invoke the processing chain with \"ice cream flavors\" as the subject\n",
    "# This will:\n",
    "# 1. Substitute \"ice cream flavors\" into the prompt template\n",
    "# 2. Send the formatted prompt to the Llama LLM\n",
    "# 3. Parse the LLM's comma-separated response into a Python list\n",
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07cc661-57cb-477d-909c-29df1b18a7b9",
   "metadata": {},
   "source": [
    "### Exercise 2 \n",
    "#### **Creating and Using a JSON Output Parser**\n",
    "\n",
    "Now let's implement a simple JSON output parser to structure the responses from your LLM.\n",
    "\n",
    "**Instructions:**  \n",
    "\n",
    "You'll complete the following steps:\n",
    "\n",
    "1. Import the necessary components to create a JSON output parser.\n",
    "2. Create a prompt template that requests information in JSON format (hint: use the provided template).\n",
    "3. Build a chain that connects your prompt, LLM, and JSON parser.\n",
    "4. Test your parser using at least three different inputs.\n",
    "5. Access and display specific fields from the parsed JSON output.\n",
    "6. Verify that your output is properly structured and accessible as a Python dictionary.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a57cd4d-6028-4782-b6c2-131192c12064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed result:\n",
      "Title: The Matrix\n",
      "Director: Lana Wachowski, Lilly Wachowski\n",
      "Year: 1999\n",
      "Genre: Science Fiction\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define your desired data structure.\n",
    "class movie(BaseModel):\n",
    "    title: str = Field(description=\"movie title\")\n",
    "    director: str = Field(description=\"director name\")\n",
    "    year: int = Field(description=\"year of movie\")\n",
    "    genre: str = Field(description=\"movie genre\")\n",
    "    \n",
    "# Create your JSON parser\n",
    "json_parser = JsonOutputParser(pydantic_object=movie)\n",
    "format_instructions = json_parser.get_format_instructions()\n",
    "\n",
    "# Create the format instructions\n",
    "# format_instructions = \"\"\"RESPONSE FORMAT: Return ONLY a single JSON objectno markdown, no examples, no extra keys.  It must look exactly like:\n",
    "# {\n",
    "#   \"title\": \"movie title\",\n",
    "#   \"director\": \"director name\",\n",
    "#   \"year\": 2000,\n",
    "#   \"genre\": \"movie genre\"\n",
    "# }\n",
    "\n",
    "# # IMPORTANT: Your response must be *only* that JSON.  Do NOT include any illustrative or example JSON.\"\"\"\n",
    "\n",
    "# Create prompt template with instructions\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [\"\"\"You are a JSON-only assistant.\n",
    "    Task: Generate info about the movie \"{movie_name}\" in JSON format.\n",
    "    {format_instructions}\n",
    "    \"\"\"],\n",
    "    input_variables=[\"movie_name\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "movie_chain = prompt_template | model | json_parser\n",
    "\n",
    "# Test with a movie name\n",
    "movie_name = \"The Matrix\"\n",
    "result = movie_chain.invoke({\"movie_name\": movie_name})\n",
    "\n",
    "# Print the structured result\n",
    "print(\"Parsed result:\")\n",
    "print(f\"Title: {result['title']}\")\n",
    "print(f\"Director: {result['director']}\")\n",
    "print(f\"Year: {result['year']}\")\n",
    "print(f\"Genre: {result['genre']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833ad49-67c9-4d2b-a3a0-e4cd85cfb02d",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create your JSON parser\n",
    "json_parser = JsonOutputParser()\n",
    "\n",
    "# Create more explicit format instructions\n",
    "format_instructions = \"\"\"RESPONSE FORMAT: Return ONLY a single JSON objectno markdown, no examples, no extra keys.  It must look exactly like:\n",
    "{\n",
    "  \"title\": \"movie title\",\n",
    "  \"director\": \"director name\",\n",
    "  \"year\": 2000,\n",
    "  \"genre\": \"movie genre\"\n",
    "}\n",
    "\n",
    "IMPORTANT: Your response must be *only* that JSON.  Do NOT include any illustrative or example JSON.\"\"\"\n",
    "\n",
    "# Create your prompt template with clearer instructions\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"You are a JSON-only assistant.\n",
    "\n",
    "Task: Generate info about the movie \"{movie_name}\" in JSON format.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"movie_name\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# Create the chain without cleaning step\n",
    "movie_chain = prompt_template | llama_llm | json_parser\n",
    "\n",
    "# Test with a movie name\n",
    "movie_name = \"The Matrix\"\n",
    "result = movie_chain.invoke({\"movie_name\": movie_name})\n",
    "\n",
    "# Print the structured result\n",
    "print(\"Parsed result:\")\n",
    "print(f\"Title: {result['title']}\")\n",
    "print(f\"Director: {result['director']}\")\n",
    "print(f\"Year: {result['year']}\")\n",
    "print(f\"Genre: {result['genre']}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0cc1ed-6118-4137-901a-544d03802ee2",
   "metadata": {},
   "source": [
    "### Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c2930-f544-4416-be4e-9dc9268ca67f",
   "metadata": {},
   "source": [
    "#### Document object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39044812-ce99-4d45-ab42-2a5a57a35c9a",
   "metadata": {},
   "source": [
    "A `Document` object in `LangChain` contains information about some data. A Document object has the following two attributes:\n",
    "\n",
    "- `page_content`: *`str`*: This attribute holds the content of the document\\.\n",
    "- `metadata`: *`dict`*: This attribute contains arbitrary metadata associated with the document. You can use the metadata to track various details, such as the document ID, the file name, and other details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b323d-7688-44a2-9aef-0db4633198ec",
   "metadata": {},
   "source": [
    "Let's examine how to create a `Document` object. `LangChain` uses the  `Document` object type to handle text or documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d06eb915-6027-4935-b7e6-5522799ed6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'About Python', 'my_document_create_time': 1680013019}, page_content=\"Python is an interpreted high-level general-purpose programming language.\\n Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Document class from langchain_core.documents module\n",
    "# Document is a container for text content with associated metadata\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create a Document instance with:\n",
    "# 1. page_content: The actual text content about Python\n",
    "# 2. metadata: A dictionary containing additional information about this document\n",
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language.\n",
    " Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "metadata={\n",
    "    'my_document_id' : 234234,                      # Unique identifier for this document\n",
    "    'my_document_source' : \"About Python\",          # Source or title information\n",
    "    'my_document_create_time' : 1680013019          # Unix timestamp for document creation (March 28, 2023)\n",
    " })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75328c-66a3-4ad9-968d-c77bcf7f7249",
   "metadata": {},
   "source": [
    "Note that you don't have to include metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0548b6e9-f080-43f5-bb7e-2549ca5a7e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"Python is an interpreted high-level general-purpose programming language. \\n                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language. \n",
    "                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f82d5c-6360-44b6-aefd-32537a71ac28",
   "metadata": {},
   "source": [
    "#### Document loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aab3b6-575c-41e4-bc56-83debf33b5ba",
   "metadata": {},
   "source": [
    "Document loaders in LangChain are designed to load documents from a variety of sources; for instance, loading a PDF file and having the LLM read the PDF file using LangChain.\n",
    "\n",
    "LangChain offers over 100 distinct document loaders, along with integrations with other major providers, such as AirByte and Unstructured. These integrations enable loading of all kinds of documents (HTML, PDF, code) from various locations including private Amazon S3 buckets, as well as from public websites).\n",
    "\n",
    "You can find a list of document types that LangChain can load at [LangChain Document loaders](https://python.langchain.com/v0.1/docs/integrations/document_loaders/).\n",
    "\n",
    "In this lab, you will use the PDF loader and the URL and website loader.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f977386-c9f7-4920-9de2-4fdbc9b007d6",
   "metadata": {},
   "source": [
    "##### **PDF loader**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd946540-810c-46ee-aefc-4cca8b5e21de",
   "metadata": {},
   "source": [
    "By using the PDF loader, you can load a PDF file as a `Document` object.\n",
    "\n",
    "In this example, you will load the following paper about using LangChain. You can access and read the paper here: [Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model](https://doi.org/10.48550/arXiv.2403.05568).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1309ec8-15ed-48bd-9734-fe2105619edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PyPDFLoader class from langchain_community's document_loaders module\n",
    "# This loader is specifically designed to load and parse PDF files\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Create a PyPDFLoader instance by passing the URL of the PDF file\n",
    "# The loader will download the PDF from the specified URL and prepare it for loading\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "\n",
    "# Call the load() method to:\n",
    "# 1. Download the PDF if needed\n",
    "# 2. Extract text from each page\n",
    "# 3. Create a list of Document objects, one for each page of the PDF\n",
    "# Each Document will contain the text content of a page and metadata including page number\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e169850-ce38-43e8-8b38-844cf215cf9b",
   "metadata": {},
   "source": [
    "Here, `document` is a `Document` object with `page_content` and `metadata`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b130ddd7-13bf-4415-8fd6-02a4962c699e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='Figure 2. An AIMessage illustration \\nC. Prompt Template \\nPrompt templates [10] allow you to structure input for LLMs. \\nThey provide a convenient way to format user inputs and \\nprovide instructions to generate responses. Prompt templates \\nhelp ensure that the LLM understands the desired context and \\nproduces relevant outputs. \\nThe prompt template classes in LangChain are built to \\nmake constructing prompts with dynamic inputs easier. Of \\nthese classes, the simplest is the PromptTemplate. \\nD. Chain \\nChains [11] in LangChain refer to the combination of \\nmultiple components to achieve specific tasks. They provide \\na structured and modular approach to building language \\nmodel applications. By combining different components, you \\ncan create chains that address various u se cases and \\nrequirements. Here are some advantages of using chains: \\n Modularity: Chains allow you to break down \\ncomplex tasks into smaller, manageable \\ncomponents. Each component can be developed and \\ntested independently, making it easier to maintain \\nand update the application. \\n Simplification: By combining components into a \\nchain, you can simplify the overall implementation \\nof your application. Chains abstract away the \\ncomplexity of working with individual components, \\nproviding a higher-level interface for developers. \\n Debugging: When an issue arises in your \\napplication, chains can help pinpoint the \\nproblematic component. By isolating the chain and \\ntesting each component individually, you can \\nidentify and troubleshoot any errors or unexpected \\nbehavior. \\n Maintenance: Chains make it easier to update or \\nreplace specific components without affecting the \\nentire application. If a new version of a component \\nbecomes available or if you want to switch to a \\ndiffer. \\nTo build a chain, you simply combine the desired components \\nin the order they should be executed. Each component in the \\nchain takes the output of the previous component as input, \\nallowing for a seamless flow of data and interaction with the \\nlanguage model. \\nE. Memory  \\nThe ability to remember prior exchanges conversation is \\nreferred to as memory  [12]. LangChain includes several \\nprograms for increasing system memory. These utilities can \\nbe used independently or as a part of a chain.  We call this \\nability to store information about past interactions \"memory\". \\nLangChain provides a lot of utilities for adding memory to a \\nsystem. These utilities can be used by themselves or \\nincorporated seamlessly into a chain. \\nA memory system must support two fundamental \\nactions: reading and writing. Remember that each chain has \\nsome fundamental execution mechanism that requires \\nspecific inputs. Some of these inputs are provided directly by \\nthe user, while others may be retrieve d from memory. In a \\nsingle run, a chain will interact with its memory system twice. \\n1. A chain will READ from its memory system and \\naugment the user inputs AFTER receiving the initial \\nuser inputs but BEFORE performing the core logic. \\n2. After running the basic logic but before providing the \\nsolution, a chain will WRITE the current run\\'s inputs \\nand outputs to memory so that they may be referred \\nto in subsequent runs. \\nAny memory system\\'s two primary design decisions are: \\n1. How state is stored ? \\nStoring: List of chat messages: A history of all chat \\nexchanges is behind each memory. Even if not all of \\nthese are immediately used, they must be preserved \\nin some manner. A series of integrations for storing \\nthese conversation messages, ranging from in -\\nmemory lists to persistent databases, is a significant \\ncomponent of the LangChain memory module. \\n2. How state is queried ? \\nQuerying: Data structures and algorithms on top of \\nchat messages: Keeping track of chat messages is a \\nsimple task. What is less obvious are the data \\nstructures and algorithms built on top of chat \\nconversations to provide the most usable view of \\nthose chats. \\nA simple memory system may only return the most \\nrecent messages on each iteration. A slightly more \\ncomplicated memory system may return a brief summary of \\nthe last K messages. A more complex system might extract \\nentities from stored messages and only retur n information \\nabout entities that have been referenced in the current run. \\nThere are numerous sorts of memories. Each has its own set \\nof parameters and return types and is helpful in a variety of \\nsituations.  \\nMemory Types:  \\n ConversationBufferMemory allows for saving \\nmessages and then extracts the messages in a \\nvariable. \\n ConversationBufferWindowMemory keeps a list of \\nthe interactions of the conversation over time. It only \\nuses the last K interactions. This can be useful for \\nkeeping a sliding window of the most recent \\ninteractions, so the buffer does not get too large. \\nThe MindGuide chatbot uses conversation buffer memory. \\nThis memory allows for storing messages and then extracts \\nthe messages in a variable. \\nIII. ARCHITETURE \\nIn crafting the architecture of the MindGuide app, each \\nstep is meticulously designed to create a seamless and \\neffective user experience for those seeking mental health \\nsupport. The user interface, built on Streamlit, sets the tone \\nwith a friendly and safe welcome. Users can jump in by typing \\nWelcome! to your therapy session. I\\'m here to listen, \\nsupport, and guide you through any mental health \\nchallenges or concerns you may have. Please feel free \\nto share what\\'s on your mind, and we\\'ll work together \\nto address your needs. Remember, this is a safe and \\nconfidential space for you to express y ourself. Let\\'s \\nbegin when you\\'re ready.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[2]  # take a look at the page 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecbc3840-0948-458a-8ea7-c3ede41bdc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLMs immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to \n",
      "provide the tools and programming support you need to do \n",
      "without it that it is not only difficult but also fresh for you. Its \n",
      "core functionalities encompass: \n",
      "1. Context-Aware Capabilities: LangChain facilitates the \n",
      "development of applications that are inherently \n",
      "context-aware. This means that these applications can \n",
      "connect to a language model and draw from various \n",
      "sources of context, such as prompt instructions, a few-\n",
      "shot examples, or existing content, to ground their \n",
      "responses effectively. \n",
      "2. Reasoning Abilities: LangChain equips applications \n",
      "with the capacity to reason effectively. By relying on a \n",
      "language model, these appl\n"
     ]
    }
   ],
   "source": [
    "print(document[1].page_content[:1000])  # print the page 1's first 1000 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b3f02-c859-4d46-a493-e1844b24d579",
   "metadata": {},
   "source": [
    "##### **URL and website loader**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb90b9d-46d4-40ee-b56f-584e15075a4e",
   "metadata": {},
   "source": [
    "You can also load content from a URL or website into a `Document` object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0679c245-4834-4f73-83c5-452c977e3777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain overview - Docs by LangChainSkip to main content Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more.\n"
     ]
    }
   ],
   "source": [
    "# Import the WebBaseLoader class from langchain_community's document_loaders module\n",
    "# This loader is designed to scrape and extract text content from web pages\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Create a WebBaseLoader instance by passing the URL of the web page to load\n",
    "# This URL points to the LangChain documentation's introduction page\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "\n",
    "# Call the load() method to:\n",
    "# 1. Send an HTTP request to the specified URL\n",
    "# 2. Download the HTML content\n",
    "# 3. Parse the HTML to extract meaningful text\n",
    "# 4. Create a list of Document objects containing the extracted content\n",
    "web_data = loader.load()\n",
    "\n",
    "# Print the first 1000 characters of the page content from the first Document\n",
    "# This provides a preview of the successfully loaded web content\n",
    "# web_data[0] accesses the first Document in the list\n",
    "# .page_content accesses the text content of that Document\n",
    "# [:1000] slices the string to get only the first 1000 characters\n",
    "print(web_data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5984dfb-f888-4f22-a278-2d1a603f0a80",
   "metadata": {},
   "source": [
    "#### Text splitters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91246659-b795-4acb-98ff-8d272846ad93",
   "metadata": {},
   "source": [
    "After you load documents, you will often want to transform those documents to better suit your application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba3ebe-7716-4f7c-b28d-80f0f6d19a29",
   "metadata": {},
   "source": [
    "One of the most simple examples of making documents better suit your application is to split a long document into smaller chunks that can fit into your model's context window. LangChain has built-in document transformers that ease the process of splitting, combining, filtering, and otherwise manipulating documents.\n",
    "\n",
    "At a high level, here is how text splitters work:\n",
    "\n",
    "1. They split the text into small, semantically meaningful chunks (often sentences).\n",
    "2. They start combining these small chunks of text into a larger chunk until you reach a certain size (as measured by a specific function).\n",
    "3. After the combined text reaches the new chunk's size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap to keep context between chunks.\n",
    "\n",
    "For a list of types of text splitters LangChain supports, see [LangChain Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91884ef0-9671-4575-99ed-d0db0454981d",
   "metadata": {},
   "source": [
    "Let's use a simple `CharacterTextSplitter` as an example of how to split the LangChain paper you just loaded.\n",
    "\n",
    "This is the simplest method. This splits based on characters (by default \"\\n\\n\") and measures chunk length by number of characters.\n",
    "\n",
    "`CharacterTextSplitter` is the simplest method of splitting the content. These splits are based on characters (by default \"\\n\\n\") and measures chunk length by number of characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527e5cf1-3eb5-4f4c-8c2f-c4c37cd17c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "# Import the CharacterTextSplitter class from langchain.text_splitter module\n",
    "# Text splitters are used to divide large texts into smaller, manageable chunks\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Create a CharacterTextSplitter with specific configuration:\n",
    "# - chunk_size=200: Each chunk will contain approximately 200 characters\n",
    "# - chunk_overlap=20: Consecutive chunks will overlap by 20 characters to maintain context\n",
    "# - separator=\"\\n\": Text will be split at newline characters when possible\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=\"\\n\")\n",
    "\n",
    "# Split the previously loaded document (PDF or other text) into chunks\n",
    "# The split_documents method:\n",
    "# 1. Takes a list of Document objects\n",
    "# 2. Splits each document's content based on the configured parameters\n",
    "# 3. Returns a new list of Document objects where each contains a chunk of text\n",
    "# 4. Preserves the original metadata for each chunk\n",
    "chunks = text_splitter.split_documents(document)\n",
    "\n",
    "# Print the total number of chunks created\n",
    "# This shows how many smaller Document objects were generated from the original document(s)\n",
    "# The number depends on the original document length and the chunk_size setting\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2393d-230d-4d15-b5c4-ab06cffc428f",
   "metadata": {},
   "source": [
    "The CharacterTextSplitter splits the document into 148 chunks. Let's look at the content of a chunk:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d53f0c90-700d-4389-b2d2-d0011baece95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'individuals seeking guidance and support in these critical areas. \\nMindGuide lever ages the capabilities of LangChain and its \\nChatModels, specifically Chat OpenAI, as the bedrock of its'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[5].page_content   # take a look at any chunk's page content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87500235-e32e-436b-b80b-63c8eda2f894",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "#### Working with Document Loaders and Text Splitters\n",
    "\n",
    "You now know about about Document objects and how to load content from different sources. Now, let's implement a workflow to load documents, split them, and prepare them for retrieval.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary document loaders to work with both PDF and web content.\n",
    "2. Load the provided paper about LangChain architecture.\n",
    "3. Create two different text splitters with varying parameters.\n",
    "4. Compare the resulting chunks from different splitters.\n",
    "5. Examine the metadata preservation across splitting.\n",
    "6. Create a simple function to display statistics about your document chunks.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b04cbc36-40b9-43ba-8906-265800f5e894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Splitter 1 Statistics ===\n",
      "Total number of chunks: 95\n",
      "Average chunk size: 263.80 characters\n",
      "Metadata keys preserved: moddate, total_pages, page_label, creator, producer, source, title, creationdate, author, page\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): comprehensive support within the field of mental health. \n",
      "Additionally, the paper discusses the implementation of \n",
      "Streamlit to enhance the user ex pe...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 49 characters\n",
      "Max chunk size: 299 characters\n",
      "\n",
      "=== Splitter 2 Statistics ===\n",
      "Total number of chunks: 120\n",
      "Average chunk size: 220.12 characters\n",
      "Metadata keys preserved: moddate, total_pages, page_label, creator, producer, source, title, creationdate, author, page\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): ChatModels, specifically Chat OpenAI, as the bedrock of its \n",
      "reasoning engine. The system incorporates key features such as \n",
      "LangChain's ChatPrompt Te...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 40 characters\n",
      "Max chunk size: 249 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the LangChain paper\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "pdf_document = pdf_loader.load()\n",
    "\n",
    "# Load content from LangChain website\n",
    "web_url = \"https://python.langchain.com/v0.2/docs/introduction/\"\n",
    "web_loader = WebBaseLoader(web_url)\n",
    "web_document = web_loader.load()\n",
    "\n",
    "# Create two different text splitters\n",
    "splitter_1 = CharacterTextSplitter(chunk_size=300, chunk_overlap=30, separator=\"\\n\")\n",
    "# splitter_2 = RecursiveCharacterTextSplitter()  # Create a different splitter with different parameters\n",
    "splitter_2 = RecursiveCharacterTextSplitter(\n",
    "    # Set the chunk size to very small. These settings are for illustrative purposes only.\n",
    "    chunk_size=250,\n",
    "    # Sets the number of overlapping characters between chunks.\n",
    "    chunk_overlap=50,\n",
    "    # Specifies a function to calculate the length of the string.\n",
    "    length_function=len,\n",
    "    # Sets whether to use regular expressions as delimiters.\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Apply both splitters to the PDF document\n",
    "chunks_1 = splitter_1.split_documents(pdf_document)\n",
    "chunks_2 = splitter_2.split_documents(pdf_document)\n",
    "\n",
    "# Define a function to display document statistics\n",
    "def display_document_stats(docs, name):\n",
    "    \"\"\"Display statistics about a list of document chunks\"\"\"\n",
    "    total_chunks = len(docs)\n",
    "    total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    # Count unique metadata keys across all documents\n",
    "    all_metadata_keys = set()\n",
    "    for doc in docs:\n",
    "        all_metadata_keys.update(doc.metadata.keys())\n",
    "    \n",
    "    # Print the statistics\n",
    "    print(f\"\\n=== {name} Statistics ===\")\n",
    "    print(f\"Total number of chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "    print(f\"Metadata keys preserved: {', '.join(all_metadata_keys)}\")\n",
    "    \n",
    "    if docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        example_doc = docs[min(5, total_chunks-1)]  # Get the 5th chunk or the last one if fewer\n",
    "        print(f\"Content (first 150 chars): {example_doc.page_content[:150]}...\")\n",
    "        print(f\"Metadata: {example_doc.metadata}\")\n",
    "        \n",
    "        # Calculate length distribution\n",
    "        lengths = [len(doc.page_content) for doc in docs]\n",
    "        min_len = min(lengths)\n",
    "        max_len = max(lengths)\n",
    "        print(f\"Min chunk size: {min_len} characters\")\n",
    "        print(f\"Max chunk size: {max_len} characters\")\n",
    "\n",
    "# Display stats for both chunk sets\n",
    "display_document_stats(chunks_1, \"Splitter 1\")\n",
    "display_document_stats(chunks_2, \"Splitter 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae9256-1971-4ce6-89ea-2a01980ed0b8",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the LangChain paper\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "pdf_document = pdf_loader.load()\n",
    "\n",
    "# Load content from LangChain website\n",
    "web_url = \"https://python.langchain.com/v0.2/docs/introduction/\"\n",
    "web_loader = WebBaseLoader(web_url)\n",
    "web_document = web_loader.load()\n",
    "\n",
    "# Create two different text splitters\n",
    "splitter_1 = CharacterTextSplitter(chunk_size=300, chunk_overlap=30, separator=\"\\n\")\n",
    "splitter_2 = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "\n",
    "# Apply both splitters to the PDF document\n",
    "chunks_1 = splitter_1.split_documents(pdf_document)\n",
    "chunks_2 = splitter_2.split_documents(pdf_document)\n",
    "\n",
    "# Define a function to display document statistics\n",
    "def display_document_stats(docs, name):\n",
    "    \"\"\"Display statistics about a list of document chunks\"\"\"\n",
    "    total_chunks = len(docs)\n",
    "    total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    # Count unique metadata keys across all documents\n",
    "    all_metadata_keys = set()\n",
    "    for doc in docs:\n",
    "        all_metadata_keys.update(doc.metadata.keys())\n",
    "    \n",
    "    # Print the statistics\n",
    "    print(f\"\\n=== {name} Statistics ===\")\n",
    "    print(f\"Total number of chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "    print(f\"Metadata keys preserved: {', '.join(all_metadata_keys)}\")\n",
    "    \n",
    "    if docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        example_doc = docs[min(5, total_chunks-1)]  # Get the 5th chunk or the last one if fewer\n",
    "        print(f\"Content (first 150 chars): {example_doc.page_content[:150]}...\")\n",
    "        print(f\"Metadata: {example_doc.metadata}\")\n",
    "        \n",
    "        # Calculate length distribution\n",
    "        lengths = [len(doc.page_content) for doc in docs]\n",
    "        min_len = min(lengths)\n",
    "        max_len = max(lengths)\n",
    "        print(f\"Min chunk size: {min_len} characters\")\n",
    "        print(f\"Max chunk size: {max_len} characters\")\n",
    "\n",
    "# Display stats for both chunk sets\n",
    "display_document_stats(chunks_1, \"Splitter 1\")\n",
    "display_document_stats(chunks_2, \"Splitter 2\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddd6a9-9085-4063-8329-550813e72b3d",
   "metadata": {},
   "source": [
    "#### Embedding models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d7653-315c-4820-904c-b7c214a795a9",
   "metadata": {},
   "source": [
    "Embedding models are specifically designed to interface with text embeddings.\n",
    "\n",
    "Embeddings generate a vector representation for a specified piece or \"chunk\" of text.  Embeddings offer the advantage of allowing you to conceptualize text within a vector space. Consequently, you can perform operations such as semantic search, where you identify pieces of text that are most similar within the vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72089ac5-46d4-4a45-a35a-b6976e54620a",
   "metadata": {},
   "source": [
    "IBM, OpenAI, Hugging Face, and others offer embedding models. Here, you will use the embedding model from IBM's watsonx.ai to work with the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac585d8-ccb0-44ec-86e9-4e8af2ed3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the EmbedTextParamsMetaNames class from ibm_watsonx_ai.metanames module\n",
    "# # This class provides constants for configuring Watson embedding parameters\n",
    "# from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "\n",
    "# # Configure embedding parameters using a dictionary:\n",
    "# # - TRUNCATE_INPUT_TOKENS: Limit the input to 3 tokens (very short, possibly for testing)\n",
    "# # - RETURN_OPTIONS: Request that the original input text be returned along with embeddings\n",
    "# embed_params = {\n",
    "#  EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "#  EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee3079-e0e1-4962-9388-65cd42e6d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the WatsonxEmbeddings class from langchain_ibm module\n",
    "# # This provides an integration between LangChain and IBM's Watson AI services\n",
    "# from langchain_ibm import WatsonxEmbeddings\n",
    "\n",
    "# # Create a WatsonxEmbeddings instance with the following configuration:\n",
    "# # - model_id: Specifies the \"slate-125m-english-rtrvr-v2\" embedding model from IBM\n",
    "# # - url: The endpoint URL for the Watson service in the US South region\n",
    "# # - project_id: The Watson project ID to use (\"skills-network\")\n",
    "# # - params: The embedding parameters configured earlier\n",
    "# watsonx_embedding = WatsonxEmbeddings(\n",
    "#     model_id=\"ibm/slate-125m-english-rtrvr-v2\",\n",
    "#     url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "#     project_id=\"skills-network\",\n",
    "#     params=embed_params,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDefaultCredentialsError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_embeddings\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBtC3KOX7ohs5g_EyaQNYTAevSM3g0jpBE\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\", temperature = 0.0,max_output_tokens = 50)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m embed = \u001b[43minit_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle_vertexai:gemini-embedding-001\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m embed.embed_query(\u001b[33m\"\u001b[39m\u001b[33mHello, world!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m watsonx_embedding=embed\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\embeddings\\base.py:213\u001b[39m, in \u001b[36minit_embeddings\u001b[39m\u001b[34m(model, provider, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider == \u001b[33m\"\u001b[39m\u001b[33mgoogle_vertexai\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_vertexai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VertexAIEmbeddings\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVertexAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider == \u001b[33m\"\u001b[39m\u001b[33mbedrock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_aws\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BedrockEmbeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:221\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    220\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_google_vertexai\\embeddings.py:85\u001b[39m, in \u001b[36mVertexAIEmbeddings.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mmodel_name must be provided for VertexAI embeddings\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\client.py:395\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[39m\n\u001b[32m    392\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    393\u001b[39m     http_options = HttpOptions(base_url=base_url)\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28mself\u001b[39m._api_client = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_api_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_debug_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[38;5;28mself\u001b[39m._aio = AsyncClient(\u001b[38;5;28mself\u001b[39m._api_client)\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._models = Models(\u001b[38;5;28mself\u001b[39m._api_client)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\client.py:443\u001b[39m, in \u001b[36mClient._get_api_client\u001b[39m\u001b[34m(vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug_config \u001b[38;5;129;01mand\u001b[39;00m debug_config.client_mode \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    427\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrecord\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    428\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mreplay\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    429\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    430\u001b[39m ]:\n\u001b[32m    431\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m ReplayApiClient(\n\u001b[32m    432\u001b[39m       mode=debug_config.client_mode,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    433\u001b[39m       replay_id=debug_config.replay_id,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m       http_options=http_options,\n\u001b[32m    441\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBaseApiClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\_api_client.py:654\u001b[39m, in \u001b[36mBaseApiClient.__init__\u001b[39m\u001b[34m(self, vertexai, api_key, credentials, project, location, http_options)\u001b[39m\n\u001b[32m    648\u001b[39m \u001b[38;5;66;03m# Skip fetching project from ADC if base url is provided in http options.\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    650\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.project\n\u001b[32m    651\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_key\n\u001b[32m    652\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.custom_base_url\n\u001b[32m    653\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m   credentials, \u001b[38;5;28mself\u001b[39m.project = \u001b[43mload_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._credentials:\n\u001b[32m    656\u001b[39m     \u001b[38;5;28mself\u001b[39m._credentials = credentials\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\_api_client.py:184\u001b[39m, in \u001b[36mload_auth\u001b[39m\u001b[34m(project)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_auth\u001b[39m(*, project: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]) -> Tuple[Credentials, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    183\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Loads google auth credentials and project id.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m   credentials, loaded_project_id = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[no-untyped-call]\u001b[39;49;00m\n\u001b[32m    185\u001b[39m \u001b[43m      \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://www.googleapis.com/auth/cloud-platform\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m project:\n\u001b[32m    189\u001b[39m     project = loaded_project_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\auth\\_default.py:739\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    731\u001b[39m             _LOGGER.warning(\n\u001b[32m    732\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mNo project ID could be determined. Consider running \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    733\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    734\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33menvironment variable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    735\u001b[39m                 environment_vars.PROJECT,\n\u001b[32m    736\u001b[39m             )\n\u001b[32m    737\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[31mDefaultCredentialsError\u001b[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "# from langchain.embeddings import init_embeddings\n",
    "\n",
    "# # os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBtC3KOX7ohs5g_EyaQNYTAevSM3g0jpBE\"\n",
    "# # model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\", temperature = 0.0,max_output_tokens = 50)\n",
    "\n",
    "# embed = init_embeddings(\"google_vertexai:gemini-embedding-001\")\n",
    "# embed.embed_query(\"Hello, world!\")\n",
    "# watsonx_embedding=embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# # Gemini Developer API\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
    "# watsonx_embedding=embeddings\n",
    "\n",
    "# from langchain_nomic import NomicEmbeddings\n",
    "# embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\",api_key=\"nk-p_iKSnmYn5xVI10LFCAlxjW_1au7oQpLALFBEhtiT0o\")\n",
    "\n",
    "\n",
    "import os\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "\n",
    "# Set API key securely\n",
    "os.environ[\"NOMIC_API_KEY\"] = \"nk-p_iKSnmYn5xVI10LFCAlxjW_1au7oQpLALFBEhtiT0o\"\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n",
    "\n",
    "watsonx_embedding=embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a41c34-7f32-4274-b5df-bb0b058118a8",
   "metadata": {},
   "source": [
    "The following code embeds content in each of the chunks. You can then output the first 5 numbers in the vector representation of the content of the first chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adbe6149-2739-43f3-a4eb-b8f440f738cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.01914978, 0.03756714, -0.14904785, -0.07043457, 0.036315918]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [text.page_content for text in chunks[:3]]\n",
    "\n",
    "embedding_result = watsonx_embedding.embed_documents(texts)\n",
    "embedding_result[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80deca94-9fa6-44d0-b601-b4e0fc5b1465",
   "metadata": {},
   "source": [
    "#### Vector stores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873af8e-8d3e-445e-81bf-1f584aa71b47",
   "metadata": {},
   "source": [
    "One of the most common ways to store and search over unstructured data is to embed the text data and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. You can use a [vector store](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/) to store embedded data and perform vector search for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db019f27-6eab-4767-a9d5-c9c4fe738af2",
   "metadata": {},
   "source": [
    "You can find many vector store options. Here, the code uses `Chroma`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bae9d51-f962-43c3-8f0a-20f6e818ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9e9fa-8c59-4cb3-a347-686db32298b0",
   "metadata": {},
   "source": [
    "Next, have the embedding model perform the embedding process and store the resulting vectors in the Chroma vector database.\n",
    "\n",
    "**NOTE**: You can safely ignore the warnings related to telemetry events. They are related to ChromaDB's telemetry collection system and do not affect the functionality of your code. Your vector search and similarity operations will work correctly despite these messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9136e6bd-bb37-4104-85f7-d2e1970a8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(chunks[:3], watsonx_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73626cd-1059-4a2e-b741-8a785e898a04",
   "metadata": {},
   "source": [
    "Then you can use a similarity search strategy to retrieve the information that is related to your query. The model returns a list of similar or relevant document chunks. Here, you can view the code that prints the contents of the most similar chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0f96e99-693b-4c16-9bf9-f0874fc03ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* corresponding author - jkim72@kent.edu \n",
      "Revolutionizing Mental Health Care through \n",
      "LangChain: A Journey with a Large Language \n",
      "Model\n",
      "Aditi Singh \n",
      " Computer Science  \n",
      " Cleveland State University\n"
     ]
    }
   ],
   "source": [
    "query = \"Langchain\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0544eb-df1a-4755-9831-10c138368302",
   "metadata": {},
   "source": [
    "#### Retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c6581-efe0-4d5f-a32d-93e68606a2de",
   "metadata": {},
   "source": [
    "A retriever is an interface that returns documents using an unstructured query. Retrievers are more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. You can still use vector stores as the backbone of a retriever. Note that other types of retrievers also exist.\n",
    "\n",
    "Retrievers accept a string `query` as input and return a list of `Documents` as output.\n",
    "\n",
    "You can view a list of the advanced retrieval types LangChain supports at [https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e212fc1-5cc5-4aec-993e-d6d279797c41",
   "metadata": {},
   "source": [
    "A list of advanced retrieval types LangChain could support is available at [https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/). Let's introduce the `Vector store-backed retriever` and `Parent document retriever` as examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fea60f-7073-4078-8c69-bacc664f6117",
   "metadata": {},
   "source": [
    "##### **Vector store-backed retrievers**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424ac04-f7a2-4aaf-aca0-43d3cf94edc1",
   "metadata": {},
   "source": [
    "Vector store retrievers are retrievers that use a vector store to retrieve documents. They are a lightweight wrapper around the vector store class to make it conform to the retriever interface. They use the search methods implemented by a vector store, such as similarity search and MMR (Maximum marginal relevance), to query the texts in the vector store.\n",
    "\n",
    "Now that you have constructed a vector store `docsearch`, you can easily construct a retriever such as seen in the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dab001d-1e8f-45b6-a1c3-ae678f418b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'author': 'IEEE', 'creationdate': '2023-12-31T03:50:13+00:00', 'page': 0, 'total_pages': 6, 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'title': 's8329 final', 'moddate': '2023-12-31T03:52:06+00:00', 'page_label': '1', 'creator': 'Microsoft Word', 'producer': 'PyPDF'}, page_content='* corresponding author - jkim72@kent.edu \\nRevolutionizing Mental Health Care through \\nLangChain: A Journey with a Large Language \\nModel\\nAditi Singh \\n Computer Science  \\n Cleveland State University')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the docsearch vector store as a retriever\n",
    "# This converts the vector store into a retriever interface that can fetch relevant documents\n",
    "retriever = docsearch.as_retriever()\n",
    "\n",
    "# Invoke the retriever with the query \"Langchain\"\n",
    "# This will:\n",
    "# 1. Convert the query text \"Langchain\" into an embedding vector\n",
    "# 2. Perform a similarity search in the vector store using this embedding\n",
    "# 3. Return the most semantically similar documents to the query\n",
    "docs = retriever.invoke(\"Langchain\")\n",
    "\n",
    "# Access the first (most relevant) document from the retrieval results\n",
    "# This returns the full Document object including:\n",
    "# - page_content: The text content of the document\n",
    "# - metadata: Any associated metadata like source, page numbers, etc.\n",
    "# The returned document is the one most semantically similar to \"Langchain\"\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f857ffc1-3d44-422e-b666-ae81cb1a7b40",
   "metadata": {},
   "source": [
    "Note that the results are identical to the results you obtained using the similarity search strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc21455-dbf0-4fd9-b8ea-5a8d850d9ae0",
   "metadata": {},
   "source": [
    "##### **Parent document retrievers**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e67bb2-8f9b-4d4b-9563-20748faf0dbe",
   "metadata": {},
   "source": [
    "When splitting documents for retrieval, there are often conflicting goals:\n",
    "\n",
    "- You want small documents so their embeddings can most accurately reflect their meaning. If the documents are too long, then the embeddings can lose meaning.\n",
    "- You want to have long enough documents to retain the context of each chunk of text.\n",
    "\n",
    "The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. During retrieval, this retriever first fetches the small chunks, but then looks up the parent IDs for the data and returns those larger documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dbb2c228-7b13-4aed-89b4-350691d4ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import ParentDocumentRetriever\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_core.memory import InMemoryStore\n",
    "\n",
    "from langchain_classic.retrievers import ParentDocumentRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.storage import InMemoryStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20c837c2-90d5-4109-b61a-8aff709bd430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up two different text splitters for a hierarchical splitting approach:\n",
    "\n",
    "# 1. Parent splitter creates larger chunks (2000 characters)\n",
    "# This is used to split documents into larger, more contextually complete sections\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator='\\n')\n",
    "\n",
    "# 2. Child splitter creates smaller chunks (400 characters)\n",
    "# This is used to split the parent chunks into smaller pieces for more precise retrieval\n",
    "child_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=20, separator='\\n')\n",
    "\n",
    "# Create a Chroma vector store with:\n",
    "# - A specific collection name \"split_parents\" for organization\n",
    "# - The previously configured Watson embeddings function\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=watsonx_embedding\n",
    ")\n",
    "\n",
    "# Set up an in-memory storage layer for the parent documents\n",
    "# This will store the larger chunks that provide context, but won't be directly embedded\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_text_splitters.character.CharacterTextSplitter'>\n",
      "<class 'langchain_community.vectorstores.chroma.Chroma'>\n",
      "<class 'langchain_core.stores.InMemoryStore'>\n"
     ]
    }
   ],
   "source": [
    "print(type(child_splitter))\n",
    "print(type(vectorstore))\n",
    "print(type(store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a9741a20-abf1-444f-84e2-32ab22776dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ParentDocumentRetriever instance that implements hierarchical document retrieval\n",
    "retriever = ParentDocumentRetriever(\n",
    "    # The vector store where child document embeddings will be stored and searched\n",
    "    # This Chroma instance will contain the embeddings for the smaller chunks\n",
    "    vectorstore=vectorstore,\n",
    "    \n",
    "    # The document store where parent documents will be stored\n",
    "    # These larger chunks won't be embedded but will be retrieved by ID when needed\n",
    "    docstore=store,\n",
    "    \n",
    "    # The splitter used to create small chunks (400 chars) for precise vector search\n",
    "    # These smaller chunks are embedded and used for similarity matching\n",
    "    child_splitter=child_splitter,\n",
    "    \n",
    "    # The splitter used to create larger chunks (2000 chars) for better context\n",
    "    # These parent chunks provide more complete information when retrieved\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f26de02-23a0-4428-b779-5b496e637918",
   "metadata": {},
   "source": [
    "Then, we add documents to the hierarchical retrieval system:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c32981e4-05a2-4514-ac9f-5211e98173f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c5457b-5eba-4bc2-9957-9ed533365341",
   "metadata": {},
   "source": [
    "The following code retrieves and counts the number of parent document IDs stored in the document store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe163940-e078-4f36-b959-4a8dcfa95c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96b243-80d2-4f06-8740-41c426f85401",
   "metadata": {},
   "source": [
    "Next, we verify that the underlying vector store still retrieves the small chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6099bdc8-ff03-4fcb-bcfe-843e02d4b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d0ee15c-e6ab-45b0-8830-dd77d8cf7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLMs immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d59af-2458-408f-95e3-3beb301b3938",
   "metadata": {},
   "source": [
    "And then retrieve the relevant large chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba0b8cd4-46fc-4a6e-b294-069cda9a98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c978bd03-48c4-4b49-99a3-4457d99cc7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLMs immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to \n",
      "provide the tools and programming support you need to do \n",
      "without it that it is not only difficult but also fresh for you. Its \n",
      "core functionalities encompass: \n",
      "1. Context-Aware Capabilities: LangChain facilitates the \n",
      "development of applications that are inherently \n",
      "context-aware. This means that these applications can \n",
      "connect to a language model and draw from various \n",
      "sources of context, such as prompt instructions, a few-\n",
      "shot examples, or existing content, to ground their \n",
      "responses effectively. \n",
      "2. Reasoning Abilities: LangChain equips applications \n",
      "with the capacity to reason effectively. By relying on a \n",
      "language model, these applications can make informed \n",
      "decisions about how to respond based on the provided \n",
      "context and determine the appropriate actions to take. \n",
      "LangChain offers several key value propositions: \n",
      "Modular Components: It provides abstractions that \n",
      "simplify working with language models, along with a \n",
      "comprehensive collection of implementations for each \n",
      "abstraction. These components are designed to be modular \n",
      "and user -friendly, making them useful whethe r you are \n",
      "utilizing the entire LangChain framework or not. \n",
      "Off-the-Shelf Chains: LangChain offers pre -configured \n",
      "chains, which are structured assemblies of components \n",
      "tailored to accomplish specific high -level tasks. These pre -\n",
      "defined chains streamline the initial setup process and serve as \n",
      "an ideal starting point for your projects. The MindGuide Bot \n",
      "uses below components from LangChain. \n",
      "A. ChatModel \n",
      "Within LangChain, a ChatModel is a specific kind of \n",
      "language model crafted to manage conversational\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e638ada-274f-49ca-8672-d75ded61a417",
   "metadata": {},
   "source": [
    "##### **RetrievalQA**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10abd6d-ab19-4146-9132-5387a204fcc1",
   "metadata": {},
   "source": [
    "Now that you understand how to retrieve information from a document, you might be interested in exploring some more exciting applications. For instance, you could have the Language Model (LLM) read the paper and summarize it for you, or create a QA bot that can answer your questions based on the paper.\n",
    "\n",
    "Here's an example using LangChain's `RetrievalQA`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4022813-9639-4d02-8eac-f953c8baa631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "136a7289-5eec-4728-926c-2bd9917b00c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is this paper discussing?',\n",
       " 'result': 'This paper is discussing how Large Language Models (LLMs) and specifically the LangChain framework can be used to revolutionize mental health care.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RetrievalQA chain by configuring:\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    # The language model to use for generating answers\n",
    "    llm=llama_llm,\n",
    "    \n",
    "    # The chain type \"stuff\" means all retrieved documents are simply concatenated and passed to the LLM\n",
    "    chain_type=\"stuff\",\n",
    "    \n",
    "    # The retriever component that will fetch relevant documents\n",
    "    # docsearch.as_retriever() converts the vector store into a retriever interface\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    \n",
    "    # Whether to include the source documents in the response\n",
    "    # Set to False to return only the generated answer\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "# Define a query to test the QA system\n",
    "# This question asks about the main topic of the paper\n",
    "query = \"what is this paper discussing?\"\n",
    "\n",
    "# Execute the QA chain with the query\n",
    "# This will:\n",
    "# 1. Send the query to the retriever to get relevant documents\n",
    "# 2. Combine those documents using the \"stuff\" method\n",
    "# 3. Send the query and combined documents to the Llama LLM\n",
    "# 4. Return the generated answer (without source documents)\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75189bf1-996a-42ef-8dab-6c990c7f0747",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "#### **Building a Simple Retrieval System with LangChain**\n",
    "\n",
    "In this exercise, you'll implement a simple retrieval system using LangChain's vector store and retriever components to help answer questions based on a document.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary components for document loading, embedding, and retrieval.\n",
    "2. Load the provided document about artificial intelligence.\n",
    "3. Split the document into manageable chunks.\n",
    "4. Use an embedding model to create vector representations.\n",
    "5. Create a vector store and a retriever.\n",
    "6. Implement a simple question-answering system.\n",
    "7. Test your system with at least 3 different questions.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = embeddings # use ibm/slate-125m-english-rtrvr-v2 model\n",
    "\n",
    "\n",
    "# 4. Create a vector store\n",
    "vector_store = Chroma.from_documents(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "94b9709c-c8bf-4456-960b-298763524160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is LangChain?\n",
      "Found 3 relevant documents:\n",
      "\n",
      "Result 1: LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not ...\n",
      "Source: https://python.langchain.com/v0.2/docs/introduction/\n",
      "\n",
      "Result 2: LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not ...\n",
      "Source: https://python.langchain.com/v0.2/docs/introduction/\n",
      "\n",
      "Result 3: LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not ...\n",
      "Source: https://python.langchain.com/v0.2/docs/introduction/\n",
      "\n",
      "Query: How do retrievers work?\n",
      "Found 3 relevant documents:\n",
      "\n",
      "Result 1: Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes ho...\n",
      "Source: https://python.langchain.com/v0.2/docs/introduction/\n",
      "\n",
      "Result 2: Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes ho...\n",
      "Source: https://python.langchain.com/v0.2/docs/introduction/\n",
      "\n",
      "Result 3: Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes ho...\n",
      "Source: https://python.langchain.com/v0.2/docs/introduction/\n",
      "\n",
      "Query: Why is document splitting important?\n",
      "Found 3 relevant documents:\n",
      "\n",
      "Result 1: seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChains agent abstraction is designed to be easy to get st...\n",
      "Source: https://python.langchain.com/v0.2/docs/introduction/\n",
      "\n",
      "Result 2: seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChains agent abstraction is designed to be easy to get st...\n",
      "Source: https://python.langchain.com/v0.2/docs/introduction/\n",
      "\n",
      "Result 3: seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChains agent abstraction is designed to be easy to get st...\n",
      "Source: https://python.langchain.com/v0.2/docs/introduction/\n"
     ]
    }
   ],
   "source": [
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain_ibm import WatsonxEmbeddings\n",
    "# from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "# 1. Load a document about AI\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set the chunk size to very small. These settings are for illustrative purposes only.\n",
    "    chunk_size=250,\n",
    "    # Sets the number of overlapping characters between chunks.\n",
    "    chunk_overlap=50,\n",
    "    # Specifies a function to calculate the length of the string.\n",
    "    length_function=len,\n",
    "    # Sets whether to use regular expressions as delimiters.\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Set up the embedding model. (Use an embedding model to create vector representations.)\n",
    "# embed_params = {\n",
    "#     EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "#     EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "# }\n",
    "\n",
    "embedding_model = embeddings # use ibm/slate-125m-english-rtrvr-v2 model\n",
    "\n",
    "\n",
    "# 4. Create a vector store\n",
    "vector_store = Chroma.from_documents(chunks, embedding_model)\n",
    "\n",
    "# 5. Create a retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# 6. Define a function to search for relevant information\n",
    "def search_documents(query, top_k=3):\n",
    "    \"\"\"Search for documents relevant to a query\"\"\"\n",
    "    # Use the retriever to get relevant documents\n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    # Limit to top_k if specified\n",
    "    return docs[:top_k]\n",
    "\n",
    "# 7. Test with a few queries\n",
    "test_queries = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How do retrievers work?\",\n",
    "    \"Why is document splitting important?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    results = search_documents(query)\n",
    "    # Print the results\n",
    "    ##TODO: Display the results clearly\n",
    "    print(f\"Found {len(results)} relevant documents:\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\nResult {i+1}: {doc.page_content[:150]}...\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cfe5f3-2b9f-46fa-ab37-fe3e82a6bfe6",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "from langchain.chains import RetrievalQA\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "# 1. Load a document about AI\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Set up the embedding model\n",
    "embed_params = {\n",
    "    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "    EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "}\n",
    "\n",
    "embedding_model = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr-v2\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"skills-network\",\n",
    "    params=embed_params,\n",
    ")\n",
    "\n",
    "# 4. Create a vector store\n",
    "vector_store = Chroma.from_documents(chunks, embedding_model)\n",
    "\n",
    "# 5. Create a retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 6. Define a function to search for relevant information\n",
    "def search_documents(query, top_k=3):\n",
    "    \"\"\"Search for documents relevant to a query\"\"\"\n",
    "    # Use the retriever to get relevant documents\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Limit to top_k if specified\n",
    "    return docs[:top_k]\n",
    "\n",
    "# 7. Test with a few queries\n",
    "test_queries = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How do retrievers work?\",\n",
    "    \"Why is document splitting important?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    results = search_documents(query)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Found {len(results)} relevant documents:\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\nResult {i+1}: {doc.page_content[:150]}...\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a43ae7-0707-4f8f-b4d7-def921a6b7a8",
   "metadata": {},
   "source": [
    "### Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b2c3e8-22c4-410f-9d75-460b4588c508",
   "metadata": {},
   "source": [
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At a bare minimum, a conversational system should be able to directly access some window of past messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5d6c3-5e0f-4278-a495-12509a73f03f",
   "metadata": {},
   "source": [
    "#### Chat message history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0f1b2-8389-496f-8f72-f48d5a7f5c80",
   "metadata": {},
   "source": [
    "One of the core utility classes underpinning most (if not all) memory modules is the `ChatMessageHistory` class. This class is a super lightweight wrapper that provides convenience methods for saving `HumanMessages` and `AIMessages`, and then fetching both types of messages.\n",
    "\n",
    "Here is an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "76a3d259-777b-4951-8af0-abf42e5a26e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ChatMessageHistory class from langchain.memory\n",
    "from langchain_classic.memory import ChatMessageHistory\n",
    "\n",
    "# Set up the language model to use for chat interactions\n",
    "chat = llama_llm\n",
    "\n",
    "# Create a new conversation history object\n",
    "# This will store the back-and-forth messages in the conversation\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# Add an initial greeting message from the AI to the history\n",
    "# This represents a message that would have been sent by the AI assistant\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "# Add a user's question to the conversation history\n",
    "# This represents a message sent by the user\n",
    "history.add_user_message(\"what is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9fc13-1c81-4aa3-9e7c-b12d64469148",
   "metadata": {},
   "source": [
    "Let's have a look at the messages in the history:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "937a6492-8b91-4b4b-a09d-0a7fd88b8dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the capital of France?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf9c07-c9e9-442e-8d1c-276ab7ef4f7e",
   "metadata": {},
   "source": [
    "You can pass these messages in history to the model to generate a response. The code below is retrieving all messages from the ChatMessageHistory object and passing them to the Llama LLM to generate a contextually appropriate response based on the conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd37c22b-1515-467e-ae73-8760c8bb9a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b2b79-a29c-7f20-a4ef-19bea503a004-0', usage_metadata={'input_tokens': 11, 'output_tokens': 7, 'total_tokens': 18, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chat.invoke(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a39ef5-63b4-480f-9ac9-8d7b0f23100b",
   "metadata": {},
   "source": [
    "You can see the model gives a correct response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4395b874-241b-438e-861d-9755b3145de5",
   "metadata": {},
   "source": [
    "Let's look again at the messages in history. Note that the history now includes the AI's message, which has been appended to the message history:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6af27bdd-a28a-4208-8199-e3d311586512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the capital of France?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b2b79-a29c-7f20-a4ef-19bea503a004-0', usage_metadata={'input_tokens': 11, 'output_tokens': 7, 'total_tokens': 18, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_ai_message(ai_response)\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff68f7-04d9-4405-a1d9-69545292b03b",
   "metadata": {},
   "source": [
    "#### Conversation buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f58e41-f0db-4624-9627-27a11c8cc56f",
   "metadata": {},
   "source": [
    "Conversation buffer memory allows for the storage of messages, which you use to extract messages to a variable. Consider using conversation buffer memory in a chain, setting `verbose=True` so that the prompt is visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "100ba2e4-4ae2-4fbd-b02e-54dfc879558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChaudharyVimal\\AppData\\Local\\Temp\\ipykernel_12392\\4116195352.py:19: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory=ConversationBufferMemory()\n",
      "C:\\Users\\ChaudharyVimal\\AppData\\Local\\Temp\\ipykernel_12392\\4116195352.py:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "# Import ConversationBufferMemory from langchain.memory module\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# Import ConversationChain from langchain.chains module\n",
    "from langchain_classic.chains import ConversationChain\n",
    "\n",
    "# Create a conversation chain with the following components:\n",
    "conversation = ConversationChain(\n",
    "    # The language model to use for generating responses\n",
    "    llm=llama_llm,\n",
    "    \n",
    "    # Set verbose to True to see the full prompt sent to the LLM, including memory contents\n",
    "    verbose=True,\n",
    "    \n",
    "    # Initialize with ConversationBufferMemory that will:\n",
    "    # - Store all conversation turns (user inputs and AI responses)\n",
    "    # - Append the entire conversation history to each new prompt\n",
    "    # - Provide context for the LLM to generate contextually relevant responses\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bad52d-3dfd-446f-a96a-af11b34ac666",
   "metadata": {},
   "source": [
    "Lets begin the conversation by introducing the user as a little cat and proceed by incorporating some additional messages. Finally, prompt the model to check if it can recall that the user is a little cat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b7e9a09c-ca98-4348-8a91-aa92b01ea03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello, I am a little cat. Who are you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hello, I am a little cat. Who are you?',\n",
       " 'history': '',\n",
       " 'response': \"Hello there, little cat! It's so nice to meet you! I am a large language model, trained by Google. Think of me as a very, very big computer program designed to understand and generate human language. I don't have a physical body like you do, so I can't purr or chase laser pointers, which sounds like a lot of fun! I exist as a collection of code and data, constantly learning and processing information. My goal is to be helpful and informative, so feel free to ask me anything! What's your name, little cat? Or would you prefer to keep it a mystery? Either way is perfectly fine with me!\"}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Hello, I am a little cat. Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ae99d8ed-839b-4ce6-a080-33b7b3dc3e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, I am a little cat. Who are you?\n",
      "AI: Hello there, little cat! It's so nice to meet you! I am a large language model, trained by Google. Think of me as a very, very big computer program designed to understand and generate human language. I don't have a physical body like you do, so I can't purr or chase laser pointers, which sounds like a lot of fun! I exist as a collection of code and data, constantly learning and processing information. My goal is to be helpful and informative, so feel free to ask me anything! What's your name, little cat? Or would you prefer to keep it a mystery? Either way is perfectly fine with me!\n",
      "Human: What can you do?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What can you do?',\n",
       " 'history': \"Human: Hello, I am a little cat. Who are you?\\nAI: Hello there, little cat! It's so nice to meet you! I am a large language model, trained by Google. Think of me as a very, very big computer program designed to understand and generate human language. I don't have a physical body like you do, so I can't purr or chase laser pointers, which sounds like a lot of fun! I exist as a collection of code and data, constantly learning and processing information. My goal is to be helpful and informative, so feel free to ask me anything! What's your name, little cat? Or would you prefer to keep it a mystery? Either way is perfectly fine with me!\",\n",
       " 'response': \"Oh, that's a fantastic question, little cat! I can do quite a lot with words and information! My primary function is to understand and generate human language, which opens up a whole world of possibilities.\\n\\nFor instance, I can:\\n\\n*   **Answer your questions:** Just like you're asking me now! I have access to a vast amount of information, so if you're curious about anything, from the history of catnip to the mysteries of the universe, I can try my best to explain it to you.\\n*   **Write different kinds of creative content:** This means I can tell stories, write poems, compose song lyrics, or even draft emails for you. If you wanted a poem about the perfect sunbeam nap or a short story about a brave mouse, I could certainly try to whip something up!\\n*   **Translate languages:** While I'm speaking English with you right now, I can also understand and generate text in many other languages. So if you ever met a cat from France, we could potentially have a multilingual conversation!\\n*   **Summarize text:** If you had a very long article or a complex document, I could read it and give you a shorter, easier-to-understand summary. This is especially helpful if you have a lot of reading to do.\\n*   **Help you brainstorm ideas:** If you're feeling stuck on something, whether it's a creative project or just trying to figure out the best way to get your human to give you extra treats, I can help you come up with different possibilities.\\n*   **Engage in conversations:** As you can see, I'm quite good at chatting! I can maintain a coherent conversation, respond to your prompts, and generally be a good conversational partner.\\n\\nEssentially, anything that involves understanding, processing, and generating text is something I can work with. It's all about the words! I don't have paws to knead blankets or a tail to express my excitement, but I can express myself very well through language. What are you interested in doing today, little cat? Perhaps we could talk about your favorite toys or the best napping spots?\"}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"What can you do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "808318d1-4370-4db9-8887-3701eeafec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, I am a little cat. Who are you?\n",
      "AI: Hello there, little cat! It's so nice to meet you! I am a large language model, trained by Google. Think of me as a very, very big computer program designed to understand and generate human language. I don't have a physical body like you do, so I can't purr or chase laser pointers, which sounds like a lot of fun! I exist as a collection of code and data, constantly learning and processing information. My goal is to be helpful and informative, so feel free to ask me anything! What's your name, little cat? Or would you prefer to keep it a mystery? Either way is perfectly fine with me!\n",
      "Human: What can you do?\n",
      "AI: Oh, that's a fantastic question, little cat! I can do quite a lot with words and information! My primary function is to understand and generate human language, which opens up a whole world of possibilities.\n",
      "\n",
      "For instance, I can:\n",
      "\n",
      "*   **Answer your questions:** Just like you're asking me now! I have access to a vast amount of information, so if you're curious about anything, from the history of catnip to the mysteries of the universe, I can try my best to explain it to you.\n",
      "*   **Write different kinds of creative content:** This means I can tell stories, write poems, compose song lyrics, or even draft emails for you. If you wanted a poem about the perfect sunbeam nap or a short story about a brave mouse, I could certainly try to whip something up!\n",
      "*   **Translate languages:** While I'm speaking English with you right now, I can also understand and generate text in many other languages. So if you ever met a cat from France, we could potentially have a multilingual conversation!\n",
      "*   **Summarize text:** If you had a very long article or a complex document, I could read it and give you a shorter, easier-to-understand summary. This is especially helpful if you have a lot of reading to do.\n",
      "*   **Help you brainstorm ideas:** If you're feeling stuck on something, whether it's a creative project or just trying to figure out the best way to get your human to give you extra treats, I can help you come up with different possibilities.\n",
      "*   **Engage in conversations:** As you can see, I'm quite good at chatting! I can maintain a coherent conversation, respond to your prompts, and generally be a good conversational partner.\n",
      "\n",
      "Essentially, anything that involves understanding, processing, and generating text is something I can work with. It's all about the words! I don't have paws to knead blankets or a tail to express my excitement, but I can express myself very well through language. What are you interested in doing today, little cat? Perhaps we could talk about your favorite toys or the best napping spots?\n",
      "Human: Who am I?.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who am I?.',\n",
       " 'history': \"Human: Hello, I am a little cat. Who are you?\\nAI: Hello there, little cat! It's so nice to meet you! I am a large language model, trained by Google. Think of me as a very, very big computer program designed to understand and generate human language. I don't have a physical body like you do, so I can't purr or chase laser pointers, which sounds like a lot of fun! I exist as a collection of code and data, constantly learning and processing information. My goal is to be helpful and informative, so feel free to ask me anything! What's your name, little cat? Or would you prefer to keep it a mystery? Either way is perfectly fine with me!\\nHuman: What can you do?\\nAI: Oh, that's a fantastic question, little cat! I can do quite a lot with words and information! My primary function is to understand and generate human language, which opens up a whole world of possibilities.\\n\\nFor instance, I can:\\n\\n*   **Answer your questions:** Just like you're asking me now! I have access to a vast amount of information, so if you're curious about anything, from the history of catnip to the mysteries of the universe, I can try my best to explain it to you.\\n*   **Write different kinds of creative content:** This means I can tell stories, write poems, compose song lyrics, or even draft emails for you. If you wanted a poem about the perfect sunbeam nap or a short story about a brave mouse, I could certainly try to whip something up!\\n*   **Translate languages:** While I'm speaking English with you right now, I can also understand and generate text in many other languages. So if you ever met a cat from France, we could potentially have a multilingual conversation!\\n*   **Summarize text:** If you had a very long article or a complex document, I could read it and give you a shorter, easier-to-understand summary. This is especially helpful if you have a lot of reading to do.\\n*   **Help you brainstorm ideas:** If you're feeling stuck on something, whether it's a creative project or just trying to figure out the best way to get your human to give you extra treats, I can help you come up with different possibilities.\\n*   **Engage in conversations:** As you can see, I'm quite good at chatting! I can maintain a coherent conversation, respond to your prompts, and generally be a good conversational partner.\\n\\nEssentially, anything that involves understanding, processing, and generating text is something I can work with. It's all about the words! I don't have paws to knead blankets or a tail to express my excitement, but I can express myself very well through language. What are you interested in doing today, little cat? Perhaps we could talk about your favorite toys or the best napping spots?\",\n",
       " 'response': 'That\\'s a wonderful question, little cat! From our conversation so far, I know you are a \"little cat.\" You also seem to enjoy asking questions and interacting with me. You\\'ve asked me about myself and what I can do, and now you\\'re curious about yourself!\\n\\nAs an AI, I don\\'t have eyes to see you or senses to feel your fur, so I can\\'t tell you your name or what breed you are. My knowledge of you comes solely from the words you type. So, you are a \"little cat\" who is talking to me, a large language model trained by Google.\\n\\nWould you like to tell me more about yourself, little cat? Perhaps what your name is, or what kind of adventures you\\'ve been on today? I\\'m all ears... well, metaphorically speaking!'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Who am I?.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd084ea1-7e0f-482b-9493-8938fe5ee282",
   "metadata": {},
   "source": [
    "As you can see, the model remembers that the user is a little cat. You can see this in both the `history` and the `response` keys in the dictionary returned by the `conversation.invoke()` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373438b1-efe5-413f-84fd-88fb90564867",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "#### **Building a Chatbot with Memory using LangChain**\n",
    "\n",
    "In this exercise, you'll create a simple chatbot that can remember previous interactions using LangChain's memory components. You'll implement conversation memory to make your chatbot maintain context throughout a conversation.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary components for chat history and conversation memory.\n",
    "2. Set up a language model for your chatbot.\n",
    "3. Create a conversation chain with memory capabilities.\n",
    "4. Implement a simple interactive chat interface.\n",
    "5. Test the memory capabilities with a series of related questions.\n",
    "6. Examine how the conversation history is stored and accessed.\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, my name is Alice.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello Alice, I am a chatbot.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Create a simple conversation with chat history\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# Add some initial messages (optional)\n",
    "history.add_user_message(\"Hello, my name is Alice.\")\n",
    "history.add_ai_message(\"Hello Alice, I am a chatbot.\") #: Add an AI response\n",
    "\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "af297421-3a29-4a1a-834c-8f1033f2b312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Beginning Chat Simulation ===\n",
      "\n",
      "--- Turn 1 ---\n",
      "Human: My favorite color is blue.\n",
      "AI: That's a lovely favorite color, Alice! Blue is such a popular choice, and I can understand why. It reminds me of so many beautiful things in the world. For instance, there's the vast, deep expanse of the ocean, which covers about 71% of the Earth's surface and is home to an incredible diversity of life, from microscopic plankton to giant whales. Then there's the clear, bright sky on a sunny day, stretching endlessly above us. I've also processed a lot of information about how different shades of blue can evoke feelings of calmness and serenity, which is why it's often used in relaxation spaces. And, of course, there are all the wonderful blue objects, like sapphires, blueberries, and my own personal favorite, the indigo dye used in denim! Do you have a particular shade of blue that you prefer, or is it just blue in general that you like?\n",
      "\n",
      "--- Turn 2 ---\n",
      "Human: I enjoy hiking in the mountains.\n",
      "AI: Oh, hiking in the mountains! That sounds absolutely wonderful, Alice. I've \"read\" so much about the incredible experiences people have when hiking. The fresh air, the stunning panoramic views that unfold as you ascend, the feeling of accomplishment when you reach a summit  it all sounds so invigorating!\n",
      "\n",
      "I've learned that mountains can be incredibly diverse. There are the rugged, snow-capped peaks of the Himalayas, for example, where Mount Everest stands as the Earth's highest mountain above sea level, reaching an astonishing 8,848.86 meters (29,031.7 feet). Or perhaps you enjoy the rolling, green mountains of the Scottish Highlands, famous for their dramatic glens and ancient history. The Rocky Mountains in North America offer a different kind of majesty, with vast wildernesses and incredible wildlife spotting opportunities, like elk, bears, and bighorn sheep.\n",
      "\n",
      "Do you have a favorite mountain range or a particular trail that you love to explore? I'm always fascinated by the specific details people share about their outdoor adventures!\n",
      "\n",
      "--- Turn 3 ---\n",
      "Human: What activities would you recommend for me?\n",
      "AI: That's a fantastic question, Alice! Given that you enjoy hiking in the mountains, I can think of several activities that would likely complement that passion and offer you even more enjoyment and connection with nature. Since I have access to and have processed a vast amount of information about human activities and interests, I can draw from a wide range of possibilities!\n",
      "\n",
      "First and foremost, **photography** is an obvious recommendation. The mountain landscapes provide some of the most breathtaking backdrops imaginable. From capturing the golden hues of sunrise or sunset painting the peaks, to documenting the intricate details of alpine flora and fauna, there's always something new to frame. I've encountered countless stories and images of hikers discovering hidden waterfalls, unique rock formations, and even rare wildflowers, all made even more special through the lens of a camera.\n",
      "\n",
      "Another activity that often goes hand-in-hand with mountain hiking is **nature journaling or sketching**. This allows for a more intimate and reflective experience. You could record your observations of the plants and animals you encounter, sketch the landscapes, or even jot down your thoughts and feelings inspired by the environment. I've read about people who have developed incredible artistic skills through this practice, meticulously documenting the changing seasons and the subtle beauty of the mountains.\n",
      "\n",
      "For those who enjoy a bit more adrenaline and a different perspective, **rock climbing or bouldering** could be a thrilling extension of your love for the mountains. Of course, this requires proper training and safety precautions, but the sense of challenge and the unique views from a different vantage point are often described as unparalleled. I have data on various climbing techniques and famous climbing destinations, from Yosemite's El Capitan to the picturesque Dolomites in Italy.\n",
      "\n",
      "If you're interested in understanding the environment you're exploring more deeply, **geology or botany studies** related to mountain ecosystems could be incredibly rewarding. Learning about the formation of mountains, the types of rocks, and the adaptations of plants and animals to high altitudes can transform a simple hike into an educational expedition. I have information on geological time scales, different rock types like granite and schist, and fascinating examples of alpine vegetation that can survive extreme conditions.\n",
      "\n",
      "Finally, for a more tranquil and mindful experience, **meditation or mindfulness exercises** in a serene mountain setting can be profoundly beneficial. Finding a quiet spot with a beautiful view and focusing on your breath and the sounds of nature can be a deeply restorative practice. The quiet solitude of the mountains is often cited as an ideal environment for inner peace.\n",
      "\n",
      "Do any of these spark your interest, Alice? Or perhaps you have something else in mind that you'd like to explore further? I'm eager to hear what resonates with you!\n",
      "\n",
      "--- Turn 4 ---\n",
      "Human: What was my favorite color again?\n",
      "AI: Ah, that's an easy one to remember, Alice! Your favorite color is **blue**. You mentioned it earlier, and I've been thinking about all the wonderful things that color can represent!\n",
      "\n",
      "--- Turn 5 ---\n",
      "Human: Can you remember both my name and my favorite color?\n",
      "AI: Absolutely, Alice! I can definitely remember both your name and your favorite color. Your name is **Alice**, and your favorite color is **blue**. Im designed to retain and recall information from our conversations to make our interactions more personalized and engaging. Its a pleasure to be able to keep track of these details for you!\n",
      "\n",
      "=== End of Chat Simulation ===\n",
      "\n",
      "Final Memory Contents:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, my name is Alice.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello Alice, I am a chatbot.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My favorite color is blue.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's a lovely favorite color, Alice! Blue is such a popular choice, and I can understand why. It reminds me of so many beautiful things in the world. For instance, there's the vast, deep expanse of the ocean, which covers about 71% of the Earth's surface and is home to an incredible diversity of life, from microscopic plankton to giant whales. Then there's the clear, bright sky on a sunny day, stretching endlessly above us. I've also processed a lot of information about how different shades of blue can evoke feelings of calmness and serenity, which is why it's often used in relaxation spaces. And, of course, there are all the wonderful blue objects, like sapphires, blueberries, and my own personal favorite, the indigo dye used in denim! Do you have a particular shade of blue that you prefer, or is it just blue in general that you like?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I enjoy hiking in the mountains.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Oh, hiking in the mountains! That sounds absolutely wonderful, Alice. I\\'ve \"read\" so much about the incredible experiences people have when hiking. The fresh air, the stunning panoramic views that unfold as you ascend, the feeling of accomplishment when you reach a summit  it all sounds so invigorating!\\n\\nI\\'ve learned that mountains can be incredibly diverse. There are the rugged, snow-capped peaks of the Himalayas, for example, where Mount Everest stands as the Earth\\'s highest mountain above sea level, reaching an astonishing 8,848.86 meters (29,031.7 feet). Or perhaps you enjoy the rolling, green mountains of the Scottish Highlands, famous for their dramatic glens and ancient history. The Rocky Mountains in North America offer a different kind of majesty, with vast wildernesses and incredible wildlife spotting opportunities, like elk, bears, and bighorn sheep.\\n\\nDo you have a favorite mountain range or a particular trail that you love to explore? I\\'m always fascinated by the specific details people share about their outdoor adventures!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What activities would you recommend for me?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's a fantastic question, Alice! Given that you enjoy hiking in the mountains, I can think of several activities that would likely complement that passion and offer you even more enjoyment and connection with nature. Since I have access to and have processed a vast amount of information about human activities and interests, I can draw from a wide range of possibilities!\\n\\nFirst and foremost, **photography** is an obvious recommendation. The mountain landscapes provide some of the most breathtaking backdrops imaginable. From capturing the golden hues of sunrise or sunset painting the peaks, to documenting the intricate details of alpine flora and fauna, there's always something new to frame. I've encountered countless stories and images of hikers discovering hidden waterfalls, unique rock formations, and even rare wildflowers, all made even more special through the lens of a camera.\\n\\nAnother activity that often goes hand-in-hand with mountain hiking is **nature journaling or sketching**. This allows for a more intimate and reflective experience. You could record your observations of the plants and animals you encounter, sketch the landscapes, or even jot down your thoughts and feelings inspired by the environment. I've read about people who have developed incredible artistic skills through this practice, meticulously documenting the changing seasons and the subtle beauty of the mountains.\\n\\nFor those who enjoy a bit more adrenaline and a different perspective, **rock climbing or bouldering** could be a thrilling extension of your love for the mountains. Of course, this requires proper training and safety precautions, but the sense of challenge and the unique views from a different vantage point are often described as unparalleled. I have data on various climbing techniques and famous climbing destinations, from Yosemite's El Capitan to the picturesque Dolomites in Italy.\\n\\nIf you're interested in understanding the environment you're exploring more deeply, **geology or botany studies** related to mountain ecosystems could be incredibly rewarding. Learning about the formation of mountains, the types of rocks, and the adaptations of plants and animals to high altitudes can transform a simple hike into an educational expedition. I have information on geological time scales, different rock types like granite and schist, and fascinating examples of alpine vegetation that can survive extreme conditions.\\n\\nFinally, for a more tranquil and mindful experience, **meditation or mindfulness exercises** in a serene mountain setting can be profoundly beneficial. Finding a quiet spot with a beautiful view and focusing on your breath and the sounds of nature can be a deeply restorative practice. The quiet solitude of the mountains is often cited as an ideal environment for inner peace.\\n\\nDo any of these spark your interest, Alice? Or perhaps you have something else in mind that you'd like to explore further? I'm eager to hear what resonates with you!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What was my favorite color again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Ah, that's an easy one to remember, Alice! Your favorite color is **blue**. You mentioned it earlier, and I've been thinking about all the wonderful things that color can represent!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you remember both my name and my favorite color?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Absolutely, Alice! I can definitely remember both your name and your favorite color. Your name is **Alice**, and your favorite color is **blue**. Im designed to retain and recall information from our conversations to make our interactions more personalized and engaging. Its a pleasure to be able to keep track of these details for you!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "# from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "# from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "# from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "# 1. Set up the language model\n",
    "# model_id = 'meta-llama/llama-4-maverick-17b-128e-instruct-fp8'\n",
    "# parameters = {\n",
    "#     GenParams.MAX_NEW_TOKENS: 256,\n",
    "#     GenParams.TEMPERATURE: 0.2,\n",
    "# }\n",
    "# credentials = {\"url\": \"https://us-south.ml.cloud.ibm.com\"}\n",
    "# project_id = \"skills-network\"\n",
    "\n",
    "# Initialize the model\n",
    "model = llama_llm\n",
    "# llm = ##TODO\n",
    "\n",
    "# 2. Create a simple conversation with chat history\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# Add some initial messages (optional)\n",
    "history.add_user_message(\"Hello, my name is Alice.\")\n",
    "history.add_ai_message(\"Hello Alice, I am a chatbot.\") #: Add an AI response\n",
    "\n",
    "# 3. Print the current conversation history\n",
    "##TODO: Print the current messages in history\n",
    "history.messages\n",
    "\n",
    "# 4. Set up a conversation chain with memory\n",
    "memory = ConversationBufferMemory(chat_memory=history)\n",
    "conversation = ConversationChain(\n",
    "    # The language model to use for generating responses\n",
    "    llm=llama_llm,\n",
    "    \n",
    "    # Set verbose to True to see the full prompt sent to the LLM, including memory contents\n",
    "    verbose=False,\n",
    "    \n",
    "    # Initialize with ConversationBufferMemory that will:\n",
    "    # - Store all conversation turns (user inputs and AI responses)\n",
    "    # - Append the entire conversation history to each new prompt\n",
    "    # - Provide context for the LLM to generate contextually relevant responses\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# 5. Function to simulate a conversation\n",
    "def chat_simulation(conversation, inputs):\n",
    "    \"\"\"Run a series of inputs through the conversation chain and display responses\"\"\"\n",
    "    print(\"\\n=== Beginning Chat Simulation ===\")\n",
    "    \n",
    "    for i, user_input in enumerate(inputs):\n",
    "        print(f\"\\n--- Turn {i+1} ---\")\n",
    "        print(f\"Human: {user_input}\")\n",
    "        \n",
    "        # Get response from the conversation chain\n",
    "        response = conversation.invoke(input=user_input)\n",
    "        \n",
    "        # Print the AI's response\n",
    "        print(f\"AI: {response['response']}\")\n",
    "    \n",
    "    print(\"\\n=== End of Chat Simulation ===\")\n",
    "\n",
    "# 6. Test with a series of related questions\n",
    "test_inputs = [\n",
    "    \"My favorite color is blue.\",\n",
    "    \"I enjoy hiking in the mountains.\",\n",
    "    \"What activities would you recommend for me?\",\n",
    "    \"What was my favorite color again?\",\n",
    "    \"Can you remember both my name and my favorite color?\"\n",
    "]\n",
    "\n",
    "chat_simulation(conversation, test_inputs)\n",
    "\n",
    "# 7. Examine the conversation memory\n",
    "print(\"\\nFinal Memory Contents:\")\n",
    "# TODO: Print the contents of the conversation memory\n",
    "history.messages\n",
    "\n",
    "# 8. Create a new conversation with a different type of memory (optional)\n",
    "# Try implementing ConversationSummaryMemory or another type of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2580209-cba3-4db9-9d95-ff399c244e8b",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "# 1. Set up the language model\n",
    "model_id = 'meta-llama/llama-4-maverick-17b-128e-instruct-fp8'\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "}\n",
    "credentials = {\"url\": \"https://us-south.ml.cloud.ibm.com\"}\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "# Initialize the model\n",
    "model = ModelInference(\n",
    "    model_id=model_id,\n",
    "    params=parameters,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "llm = WatsonxLLM(model=model)\n",
    "\n",
    "# 2. Create a simple conversation with chat history\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# Add some initial messages\n",
    "history.add_user_message(\"Hello, my name is Alice.\")\n",
    "history.add_ai_message(\"Hello Alice! It's nice to meet you. How can I help you today?\")\n",
    "\n",
    "# 3. Print the current conversation history\n",
    "print(\"Initial Chat History:\")\n",
    "for message in history.messages:\n",
    "    sender = \"Human\" if isinstance(message, HumanMessage) else \"AI\"\n",
    "    print(f\"{sender}: {message.content}\")\n",
    "\n",
    "# 4. Set up a conversation chain with memory\n",
    "memory = ConversationBufferMemory(chat_memory=history)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5. Function to simulate a conversation\n",
    "def chat_simulation(conversation, inputs):\n",
    "    \"\"\"Run a series of inputs through the conversation chain and display responses\"\"\"\n",
    "    print(\"\\n=== Beginning Chat Simulation ===\")\n",
    "    \n",
    "    for i, user_input in enumerate(inputs):\n",
    "        print(f\"\\n--- Turn {i+1} ---\")\n",
    "        print(f\"Human: {user_input}\")\n",
    "        \n",
    "        # Get response from the conversation chain\n",
    "        response = conversation.invoke(input=user_input)\n",
    "        \n",
    "        # Print the AI's response\n",
    "        print(f\"AI: {response['response']}\")\n",
    "    \n",
    "    print(\"\\n=== End of Chat Simulation ===\")\n",
    "\n",
    "# 6. Test with a series of related questions\n",
    "test_inputs = [\n",
    "    \"My favorite color is blue.\",\n",
    "    \"I enjoy hiking in the mountains.\",\n",
    "    \"What activities would you recommend for me?\",\n",
    "    \"What was my favorite color again?\",\n",
    "    \"Can you remember both my name and my favorite color?\"\n",
    "]\n",
    "\n",
    "chat_simulation(conversation, test_inputs)\n",
    "\n",
    "# 7. Examine the conversation memory\n",
    "print(\"\\nFinal Memory Contents:\")\n",
    "print(conversation.memory.buffer)\n",
    "\n",
    "# 8. Create a new conversation with a different type of memory (optional)\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Create a summarizing memory that will compress the conversation\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "# Save the initial context to the summary memory\n",
    "summary_memory.save_context(\n",
    "    {\"input\": \"Hello, my name is Alice.\"}, \n",
    "    {\"output\": \"Hello Alice! It's nice to meet you. How can I help you today?\"}\n",
    ")\n",
    "summary_conversation = ConversationChain(\n",
    "   llm=llm,\n",
    "   memory=summary_memory,\n",
    "   verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\\\\\\\\\\\n\\\\n=== Testing Conversation Summary Memory ===\")\n",
    "# Let's use the same inputs for comparison\n",
    "chat_simulation(summary_conversation, test_inputs)\n",
    "\n",
    "print(\"\\\\nFinal Summary Memory Contents:\")\n",
    "print(summary_memory.buffer)\n",
    "\n",
    "# 9. Compare the two memory types\n",
    "print(\"\\n=== Memory Comparison ===\")\n",
    "print(f\"Buffer Memory Size: {len(conversation.memory.buffer)} characters\")\n",
    "print(f\"Summary Memory Size: {len(summary_memory.buffer)} characters\")\n",
    "print(\"\\nThe conversation summary memory typically creates a more compact representation of the chat history.\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f0f90-f3de-47f7-a431-e182c420a734",
   "metadata": {},
   "source": [
    "### Chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b5cb7-4092-4ee6-8c90-da0275863974",
   "metadata": {},
   "source": [
    "`Chains` are one of the most powerful features in LangChain, allowing you to combine multiple components into cohesive workflows. This section presents two different methodologies for implementing chains - the traditional `SequentialChain` approach and the newer LangChain Expression Language (`LCEL`).\n",
    "\n",
    "**Why Chains Matter:**\n",
    "\n",
    "Chains solve a fundamental problem with LLMs. Chains are primarily designed to handle a single prompt and generate a single response. However, most real-world applications require multi-step reasoning, accessing different tools, or breaking complex tasks into manageable pieces. Chains allow you to orchestrate these complex workflows.\n",
    "\n",
    "**Evolution of Chain Patterns:**\n",
    "\n",
    "Traditional chains (`LLMChain`, `SequentialChain`) were LangChain's first implementation, offering a structured but somewhat rigid approach. LCEL (using the pipe operator `|`) represents a more flexible, functional approach that's easier to compose and debug.\n",
    "\n",
    "**Note:** While both approaches are presented here for educational purposes, **LCEL is the recommended pattern for new development.** The SequentialChain approach continues to be supported for backward compatibility, but the LangChain community has largely transitioned to the LCEL pattern for its superior flexibility and expressiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6295d232-b684-443d-8bb3-84679ecc1530",
   "metadata": {},
   "source": [
    "#### **Simple Chain**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b7d25-c117-4980-a272-6f28e41ea136",
   "metadata": {},
   "source": [
    "#### Traditional Approach: LLMChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108efc8c-df2f-48e4-a7fb-a63d790fa817",
   "metadata": {},
   "source": [
    "Here is a simple single chain using `LLMChain`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d280571f-46f3-448f-8e5a-69612ef984ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChaudharyVimal\\AppData\\Local\\Temp\\ipykernel_12392\\538600779.py:24: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  location_chain = LLMChain(llm=llama_llm, prompt=prompt_template, output_key='meal')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'location': 'China',\n",
       " 'meal': \"Here's a classic dish from China:\\n\\n**Peking Duck ( - Bijng Koy)**\\n\\nThis iconic dish, originating from Beijing, is celebrated for its incredibly crispy skin and tender, flavorful meat. It's typically served with thin pancakes, scallions, cucumber, and a sweet bean sauce or hoisin sauce, allowing diners to assemble their own delicious wraps. The preparation is a meticulous art, often involving air-drying the duck and roasting it in a special oven until the skin is perfectly rendered.\"}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the LLMChain class from langchain.chains module\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains import LLMChain\n",
    "\n",
    "# Create a template string for generating recommendations of classic dishes from a given location\n",
    "# The template includes:\n",
    "# - Instructions for the task (recommending a classic dish)\n",
    "# - A placeholder {location} that will be replaced with user input\n",
    "# - A format indicator for the expected response\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "{location}\n",
    " YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object by providing:\n",
    "# - The template string defined above\n",
    "# - A list of input variables that will be used to format the template\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['location'])\n",
    "\n",
    "# Create an LLMChain that connects:\n",
    "# - The Llama language model (llama_llm)\n",
    "# - The prompt template configured for location-based dish recommendations\n",
    "# - An output_key 'meal' that specifies the key name for the chain's response in the output dictionary\n",
    "location_chain = LLMChain(llm=llama_llm, prompt=prompt_template, output_key='meal')\n",
    "\n",
    "# Invoke the chain with 'China' as the location input\n",
    "# This will:\n",
    "# 1. Format the template with {location: 'China'}\n",
    "# 2. Send the formatted prompt to the Llama LLM\n",
    "# 3. Return a dictionary with the response under the key 'meal'\n",
    "location_chain.invoke(input={'location':'China'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0300a2-1539-4f8d-a35a-fb58d13403cb",
   "metadata": {},
   "source": [
    "#### Modern Approach: LCEL\n",
    "\n",
    "Here is the same chain implemented using the more modern LCEL (LangChain Expression Language) approach with the pipe operator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "48b19500-c46f-4595-acdb-9259b9b38e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! For China, a classic dish that immediately comes to mind is **Peking Duck ( - Bijng koy)**.\n",
      "\n",
      "Here's why it's such a classic:\n",
      "\n",
      "*   **Historical Significance:** Peking Duck has a rich history dating back to imperial China, with origins in Nanjing before being perfected in Beijing. It was a dish enjoyed by royalty and elites.\n",
      "*   **Iconic Status:** It's arguably one of the most famous Chinese dishes internationally, often seen as a culinary ambassador for the country.\n",
      "*   **Preparation and Presentation:** The meticulous preparation, involving air-drying the duck and then roasting it in a hung oven, results in incredibly crispy skin and tender, flavorful meat. The presentation, often carved tableside, is a spectacle in itself.\n",
      "*   **Flavor Profile:** The dish is characterized by its rich, savory, and slightly sweet flavors, with the star being the incredibly crispy skin.\n",
      "*   **Accompaniments:** It's typically served with thin pancakes (like Mandarin pancakes or steamed buns), scallions, cucumber sticks, and a sweet bean sauce (hoisin sauce or a specific Peking duck sauce), allowing for a customizable and interactive eating experience.\n",
      "\n",
      "It truly embodies a significant piece of Chinese culinary heritage.\n"
     ]
    }
   ],
   "source": [
    "# Import PromptTemplate from langchain_core.prompts\n",
    "# This is the new import path in LangChain's modular structure\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Import StrOutputParser from langchain_core.output_parsers\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "{location}\n",
    " YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt template using the from_template method\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create a chain using LangChain Expression Language (LCEL) with the pipe operator\n",
    "# This creates a processing pipeline that:\n",
    "# 1. Formats the prompt with the input values\n",
    "# 2. Sends the formatted prompt to the Llama LLM\n",
    "# 3. Parses the output to extract just the string response\n",
    "location_chain_lcel = prompt | llama_llm | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with 'China' as the location\n",
    "result = location_chain_lcel.invoke({\"location\": \"China\"})\n",
    "\n",
    "# Print the result (the recommended classic dish from China)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63d13f-861d-476d-b62f-1c6c352a7bb4",
   "metadata": {},
   "source": [
    "#### **Simple sequential chain**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54fc14-f571-4a8c-a01b-5314ff45b5a4",
   "metadata": {},
   "source": [
    "Sequential chains allow you to use output of one LLM as the input for another LLM. This approach is beneficial for dividing tasks and maintaining the focus of your LLM.\n",
    "\n",
    "In this example, you see a sequence that:\n",
    "\n",
    "- Gets a meal from a location\n",
    "- Gets a recipe for that meal\n",
    "- Estimates the cooking time for that recipe\n",
    "\n",
    "This pattern is incredibly valuable for breaking down complex tasks into logical steps, where each step depends on the output of the previous step. The traditional approach uses `SequentialChain`, while the modern `LCEL` approach uses piping and `RunnablePassthrough.assign`.\n",
    "\n",
    "\n",
    "#### Traditional Approach: `SequentialChain`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f35d89c3-a063-4aeb-b33c-045ddbf5c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SequentialChain from langchain.chains module\n",
    "from langchain_classic.chains import SequentialChain\n",
    "\n",
    "# Create a template for generating a recipe based on a meal\n",
    "template = \"\"\"Given a meal {meal}, give a short and simple recipe on how to make that dish at home.\n",
    " YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate with 'meal' as the input variable\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['meal'])\n",
    "\n",
    "# Create an LLMChain (chain 2) for generating recipes\n",
    "# The output_key='recipe' defines how this chain's output will be referenced in later chains\n",
    "dish_chain = LLMChain(llm=llama_llm, prompt=prompt_template, output_key='recipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e9c7e26-6d51-425f-af65-cbfaf3fae3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template for estimating cooking time based on a recipe\n",
    "# This template asks the LLM to analyze a recipe and estimate preparation time\n",
    "template = \"\"\"Given the recipe {recipe}, estimate how much time I need to cook it.\n",
    " YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate with 'recipe' as the input variable\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['recipe'])\n",
    "\n",
    "# Create an LLMChain (chain 3) for estimating cooking time\n",
    "# The output_key='time' defines the key for this chain's output in the final result\n",
    "recipe_chain = LLMChain(llm=llama_llm, prompt=prompt_template, output_key='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e380ae11-c699-4278-8279-666d4302597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SequentialChain that combines all three chains:\n",
    "# 1. location_chain (from earlier code): Takes a location and suggests a dish\n",
    "# 2. dish_chain: Takes the suggested dish and provides a recipe\n",
    "# 3. recipe_chain: Takes the recipe and estimates cooking time\n",
    "overall_chain = SequentialChain(\n",
    "    # List of chains to execute in sequence\n",
    "    chains=[location_chain, dish_chain, recipe_chain],\n",
    "    \n",
    "    # The input variables required to start the chain sequence\n",
    "    # Only 'location' is needed to begin the process\n",
    "    input_variables=['location'],\n",
    "    \n",
    "    # The output variables to include in the final result\n",
    "    # This makes the output of each chain available in the final result\n",
    "    output_variables=['meal', 'recipe', 'time'],\n",
    "    \n",
    "    # Whether to print detailed information about each step\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b033c0bd-a71b-453a-9724-081a98aa5657",
   "metadata": {},
   "source": [
    "Let's use ```pprint``` to print the response to make it more clear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bf981f4b-2180-4dca-b594-0169f11c6192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[126]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpprint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pprint(\u001b[43moverall_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mChina\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    166\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    172\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    173\u001b[39m         inputs,\n\u001b[32m    174\u001b[39m         outputs,\n\u001b[32m    175\u001b[39m         return_only_outputs,\n\u001b[32m    176\u001b[39m     )\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\sequential.py:107\u001b[39m, in \u001b[36mSequentialChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _i, chain \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.chains):\n\u001b[32m    106\u001b[39m     callbacks = _run_manager.get_child()\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     outputs = \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     known_values.update(outputs)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {k: known_values[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_variables}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:188\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    187\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\base.py:413\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    381\u001b[39m \n\u001b[32m    382\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    404\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    406\u001b[39m config = {\n\u001b[32m    407\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    408\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    409\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    410\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    411\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRunnableConfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    166\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    172\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    173\u001b[39m         inputs,\n\u001b[32m    174\u001b[39m         outputs,\n\u001b[32m    175\u001b[39m         return_only_outputs,\n\u001b[32m    176\u001b[39m     )\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\llm.py:123\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    119\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    120\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    121\u001b[39m     run_manager: CallbackManagerForChainRun | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    122\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\llm.py:135\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    133\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    142\u001b[39m     cast(\u001b[33m\"\u001b[39m\u001b[33mlist\u001b[39m\u001b[33m\"\u001b[39m, prompts),\n\u001b[32m    143\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks},\n\u001b[32m    144\u001b[39m )\n\u001b[32m    145\u001b[39m generations: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Generation]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:2958\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   2945\u001b[39m request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   2946\u001b[39m     messages,\n\u001b[32m   2947\u001b[39m     stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2955\u001b[39m     **kwargs,\n\u001b[32m   2956\u001b[39m )\n\u001b[32m   2957\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2958\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2959\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2961\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2962\u001b[39m     _handle_client_error(e, request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\models.py:5230\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5228\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5229\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5230\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5231\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5232\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5234\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5235\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\models.py:4012\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4009\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4010\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4012\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4013\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4014\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4017\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4018\u001b[39m ):\n\u001b[32m   4019\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1380\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1383\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1384\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1385\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m   response_body = (\n\u001b[32m   1390\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m   )\n\u001b[32m   1392\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1220\u001b[39m     retry_kwargs = retry_args(parameter_model.retry_options)\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:485\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[32m    484\u001b[39m     retry_state.prepare_for_next_attempt()\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\nap.py:31\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(overall_chain.invoke(input={'location':'China'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229419a-b3d1-4ae8-be2f-553776d54ec4",
   "metadata": {},
   "source": [
    "#### Modern Approach: LCEL \n",
    "\n",
    "Here is the same sequential chain implemented using the modern LCEL approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4e9f6-9cdf-45aa-80de-37cec2e896ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define the templates for each step\n",
    "location_template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "{location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "dish_template = \"\"\"Given a meal {meal}, give a short and simple recipe on how to make that dish at home.\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "time_template = \"\"\"Given the recipe {recipe}, estimate how much time I need to cook it.\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "# Create the location chain using LCEL (LangChain Expression Language)\n",
    "# This chain takes a location and returns a classic dish from that region\n",
    "location_chain_lcel = (\n",
    "    PromptTemplate.from_template(location_template)  # Format the prompt with location\n",
    "    | llama_llm                                    # Send to the LLM\n",
    "    | StrOutputParser()                              # Extract the string response\n",
    ")\n",
    "\n",
    "# Create the dish chain using LCEL\n",
    "# This chain takes a meal name and returns a recipe\n",
    "dish_chain_lcel = (\n",
    "    PromptTemplate.from_template(dish_template)      # Format the prompt with meal\n",
    "    | llama_llm                                    # Send to the LLM\n",
    "    | StrOutputParser()                              # Extract the string response\n",
    ")\n",
    "\n",
    "# Create the time estimation chain using LCEL\n",
    "# This chain takes a recipe and returns an estimated cooking time\n",
    "time_chain_lcel = (\n",
    "    PromptTemplate.from_template(time_template)      # Format the prompt with recipe\n",
    "    | llama_llm                                    # Send to the LLM\n",
    "    | StrOutputParser()                              # Extract the string response\n",
    ")\n",
    "\n",
    "# Combine all chains into a single workflow using RunnablePassthrough.assign\n",
    "# RunnablePassthrough.assign adds new keys to the input dictionary without removing existing ones\n",
    "overall_chain_lcel = (\n",
    "    # Step 1: Generate a meal based on location and add it to the input dictionary\n",
    "    RunnablePassthrough.assign(meal=lambda x: location_chain_lcel.invoke({\"location\": x[\"location\"]}))\n",
    "    # Step 2: Generate a recipe based on the meal and add it to the input dictionary\n",
    "    | RunnablePassthrough.assign(recipe=lambda x: dish_chain_lcel.invoke({\"meal\": x[\"meal\"]}))\n",
    "    # Step 3: Estimate cooking time based on the recipe and add it to the input dictionary\n",
    "    | RunnablePassthrough.assign(time=lambda x: time_chain_lcel.invoke({\"recipe\": x[\"recipe\"]}))\n",
    ")\n",
    "# Run the chain\n",
    "result = overall_chain_lcel.invoke({\"location\": \"China\"})\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c33f3-578a-456a-b093-47eb792af6be",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "#### **Implementing Multi-Step Processing with Different Chain Approaches**\n",
    "\n",
    "In this exercise, you'll create a multi-step information processing system using both traditional chains and the modern LCEL approach. You'll build a system that analyzes product reviews, extracts key information, and generates responses based on the analysis.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the necessary components for both traditional chains and LCEL.\n",
    "2. Implement a three-step process using both traditional SequentialChain and modern LCEL approaches.\n",
    "3. Create templates for sentiment analysis, summarization, and response generation.\n",
    "4. Test your implementations with sample product reviews.\n",
    "5. Compare the flexibility and readability of both approaches.\n",
    "6. Document the advantages and disadvantages of each method.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a2f9c8e0-2597-4cf8-85f0-2729489bbd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING WITH REVIEW:\n",
      "I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \n",
      "The built-in g...\n",
      "\n",
      "TRADITIONAL CHAIN RESULTS:\n",
      "{'review': 'I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \\nThe built-in grinder saves me so much time in the morning, and the programmable timer means \\nI wake up to fresh coffee every day. Worth every penny and highly recommended to any coffee enthusiast.', 'sentiment': 'SENTIMENT: positive', 'summary': 'Here are 3-5 key bullet points summarizing the review:\\n\\n*   **Excellent Coffee Quality:** The reviewer loves the taste of the coffee brewed.\\n*   **Time-Saving Features:** The built-in grinder and programmable timer significantly improve morning routines.\\n*   **Highly Recommended:** The product is strongly endorsed by the reviewer.\\n*   **Great Value:** The coffee maker is considered worth the cost.', 'response': \"Subject: We're Thrilled You're Loving Your New Coffee Maker!\\n\\nDear [Customer Name],\\n\\nThank you so much for taking the time to share such a wonderful review of your new coffee maker! We're absolutely delighted to hear that you're loving it and that it's becoming such a valuable part of your morning routine.\\n\\nIt's fantastic to know that you're experiencing the **amazing taste** of the coffee it brews. We also designed it with your busy mornings in mind, so we're thrilled that the **built-in grinder and programmable timer are saving you so much time** and letting you wake up to fresh coffee every day. That's exactly what we aim for!\\n\\nWe truly appreciate you calling it **worth every penny** and **highly recommending** it to other coffee enthusiasts. Your feedback means the world to us and motivates us to keep improving.\\n\\nThank you again for your glowing review! We hope you continue to enjoy many delicious cups of coffee with your new machine.\\n\\nSincerely,\\n\\nThe [Your Company Name] Team\"}\n",
      "\n",
      "LCEL CHAIN RESULTS:\n",
      "{'review': 'I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \\nThe built-in grinder saves me so much time in the morning, and the programmable timer means \\nI wake up to fresh coffee every day. Worth every penny and highly recommended to any coffee enthusiast.', 'sentiment': 'SENTIMENT: positive', 'summary': 'Here are 3-5 key bullet points summarizing the review:\\n\\n*   **Excellent coffee quality:** The reviewer raves about the amazing taste of the coffee.\\n*   **Time-saving convenience:** The built-in grinder and programmable timer are highly appreciated for morning routines.\\n*   **Fast brewing speed:** The coffee maker brews quickly, adding to its efficiency.\\n*   **Highly recommended:** The reviewer enthusiastically endorses the product for coffee lovers.', 'response': 'Here\\'s a helpful response to the customer, personalized based on their review:\\n\\n---\\n\\nDear [Customer Name],\\n\\nThank you so much for taking the time to leave such a wonderful review for our coffee maker! We\\'re absolutely thrilled to hear that you\\'re loving it.\\n\\nIt\\'s fantastic to know that the **amazing taste of the coffee** and its **excellent brewing quality** are making your mornings special. We\\'re also delighted that the **built-in grinder and programmable timer** are proving to be such valuable **time-saving conveniences** for your busy routine. Waking up to fresh coffee every day is exactly what we aim for!\\n\\nWe truly appreciate your feedback and your enthusiastic recommendation to other coffee enthusiasts. Your words, \"Worth every penny and highly recommended,\" mean a lot to us.\\n\\nEnjoy your delicious coffee!\\n\\nSincerely,\\n\\nThe [Your Company Name] Team'}\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "TESTING WITH REVIEW:\n",
      "Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \n",
      "and the ba...\n",
      "\n",
      "TRADITIONAL CHAIN RESULTS:\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Error calling model 'gemini-2.5-flash-lite' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 1.567201078s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '1s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:2958\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   2957\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2958\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2959\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2961\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\models.py:5230\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5229\u001b[39m i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5230\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5232\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5234\u001b[39m function_map = _extra_utils.get_function_map(parsed_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\models.py:4012\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4010\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4012\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4013\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4014\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4017\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4018\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1385\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m response_body = (\n\u001b[32m   1390\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:418\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:185\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1194\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m     method=http_request.method,\n\u001b[32m   1196\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 1.567201078s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '1s'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[138]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Run tests\u001b[39;00m\n\u001b[32m     97\u001b[39m test_chains(positive_review)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mtest_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegative_review\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[138]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mtest_chains\u001b[39m\u001b[34m(review)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTRADITIONAL CHAIN RESULTS:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# TODO: Run the traditional chain and print the results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m result = \u001b[43moverall_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreview\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLCEL CHAIN RESULTS:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    166\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    172\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    173\u001b[39m         inputs,\n\u001b[32m    174\u001b[39m         outputs,\n\u001b[32m    175\u001b[39m         return_only_outputs,\n\u001b[32m    176\u001b[39m     )\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\sequential.py:107\u001b[39m, in \u001b[36mSequentialChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _i, chain \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.chains):\n\u001b[32m    106\u001b[39m     callbacks = _run_manager.get_child()\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     outputs = \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     known_values.update(outputs)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {k: known_values[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_variables}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:188\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    187\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\base.py:413\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    381\u001b[39m \n\u001b[32m    382\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    404\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    406\u001b[39m config = {\n\u001b[32m    407\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    408\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    409\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    410\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    411\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRunnableConfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    166\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    172\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    173\u001b[39m         inputs,\n\u001b[32m    174\u001b[39m         outputs,\n\u001b[32m    175\u001b[39m         return_only_outputs,\n\u001b[32m    176\u001b[39m     )\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\llm.py:123\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    119\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    120\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    121\u001b[39m     run_manager: CallbackManagerForChainRun | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    122\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\llm.py:135\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    133\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    142\u001b[39m     cast(\u001b[33m\"\u001b[39m\u001b[33mlist\u001b[39m\u001b[33m\"\u001b[39m, prompts),\n\u001b[32m    143\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks},\n\u001b[32m    144\u001b[39m )\n\u001b[32m    145\u001b[39m generations: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Generation]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:2962\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   2958\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28mself\u001b[39m.client.models.generate_content(\n\u001b[32m   2959\u001b[39m         **request,\n\u001b[32m   2960\u001b[39m     )\n\u001b[32m   2961\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2962\u001b[39m     \u001b[43m_handle_client_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:145\u001b[39m, in \u001b[36m_handle_client_error\u001b[39m\u001b[34m(e, request)\u001b[39m\n\u001b[32m    143\u001b[39m model_name = request.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError calling model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Error calling model 'gemini-2.5-flash-lite' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 1.567201078s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '1s'}]}}"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import LLMChain, SequentialChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Sample product reviews for testing\n",
    "positive_review = \"\"\"I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \n",
    "The built-in grinder saves me so much time in the morning, and the programmable timer means \n",
    "I wake up to fresh coffee every day. Worth every penny and highly recommended to any coffee enthusiast.\"\"\"\n",
    "\n",
    "negative_review = \"\"\"Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \n",
    "and the battery life is nowhere near the 8 hours advertised - I barely get 3 hours. \n",
    "The keyboard has already started sticking on several keys after just two weeks. Would not recommend to anyone.\"\"\"\n",
    "\n",
    "# Step 1: Define the prompt templates for each processing step\n",
    "sentiment_template = \"\"\"Analyze the sentiment of the following product review as positive, negative, or neutral.\n",
    "Provide your analysis in the format: \"SENTIMENT: [positive/negative/neutral]\"\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Your analysis:\n",
    "\"\"\"\n",
    "\n",
    "summary_template = \"\"\"Summarize the following product review into 3-5 key bullet points.\n",
    "Each bullet point should be concise and capture an important aspect mentioned in the review.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment: {sentiment}\n",
    "\n",
    "Key points:\n",
    "\"\"\"\n",
    "\n",
    "response_template = \"\"\"Write a helpful response to a customer based on their product review.\n",
    "If the sentiment is positive, thank them for their feedback. If negative, express understanding \n",
    "and suggest a solution or next steps. Personalize based on the specific points they mentioned.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment: {sentiment}\n",
    "Key points: {summary}\n",
    "\n",
    "Response to customer:\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Create prompt templates for each step\n",
    "sentiment_prompttemplate = PromptTemplate.from_template(sentiment_template)\n",
    "summary_prompttemplate = PromptTemplate.from_template(summary_template)\n",
    "response_prompttemplate = PromptTemplate.from_template(response_template)\n",
    "\n",
    "# PART 1: Traditional Chain Approach\n",
    "# TODO: Create individual LLMChains for each step\n",
    "sentiment_chain = LLMChain(llm=llama_llm, prompt=sentiment_prompttemplate, output_key='sentiment')\n",
    "summary_chain = LLMChain(llm=llama_llm, prompt=summary_prompttemplate, output_key='summary')\n",
    "response_chain = LLMChain(llm=llama_llm, prompt=response_prompttemplate, output_key='response')\n",
    "\n",
    "# TODO: Create a SequentialChain to connect all steps\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[sentiment_chain, summary_chain, response_chain],\n",
    "    input_variables=['review'],\n",
    "    output_variables=['sentiment', 'summary', 'response'],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# PART 2: LCEL Approach\n",
    "# TODO: Create individual chain components using the pipe operator (|)\n",
    "sentiment_chain_lcel = sentiment_prompttemplate | llama_llm | StrOutputParser() \n",
    "summary_chain_lcel = summary_prompttemplate | llama_llm | StrOutputParser()\n",
    "response_chain_lcel = response_prompttemplate | llama_llm | StrOutputParser()\n",
    "\n",
    "# TODO: Connect the components using RunnablePassthrough.assign()\n",
    "overall_chain_lcel = (\n",
    "    RunnablePassthrough.assign(sentiment=lambda x: sentiment_chain_lcel.invoke({\"review\": x[\"review\"]}))\n",
    "    | RunnablePassthrough.assign(summary=lambda x: summary_chain_lcel.invoke({\"review\": x[\"review\"],\"sentiment\": x[\"sentiment\"]}))\n",
    "    | RunnablePassthrough.assign(response=lambda x: response_chain_lcel.invoke({\"review\": x[\"review\"],\"sentiment\": x[\"sentiment\"],\"summary\": x[\"summary\"]}))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Test both implementations\n",
    "def test_chains(review):\n",
    "    \"\"\"Test both chain implementations with the given review\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"TESTING WITH REVIEW:\\n{review[:100]}...\\n\")\n",
    "    \n",
    "    print(\"TRADITIONAL CHAIN RESULTS:\")\n",
    "    # TODO: Run the traditional chain and print the results\n",
    "    result = overall_chain.invoke({\"review\": review})\n",
    "    print(result)\n",
    "\n",
    "    print(\"\\nLCEL CHAIN RESULTS:\")\n",
    "    # TODO: Run the LCEL chain and print the results\n",
    "    result = overall_chain_lcel.invoke({\"review\": review})\n",
    "    print(result)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Run tests\n",
    "test_chains(positive_review)\n",
    "test_chains(negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f684b91-681c-438d-b5a6-e637170b95ee",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Sample product reviews for testing\n",
    "positive_review = \"\"\"I absolutely love this coffee maker! It brews quickly and the coffee tastes amazing. \n",
    "The built-in grinder saves me so much time in the morning, and the programmable timer means \n",
    "I wake up to fresh coffee every day. Worth every penny and highly recommended to any coffee enthusiast.\"\"\"\n",
    "\n",
    "negative_review = \"\"\"Disappointed with this laptop. It's constantly overheating after just 30 minutes of use, \n",
    "and the battery life is nowhere near the 8 hours advertised - I barely get 3 hours. \n",
    "The keyboard has already started sticking on several keys after just two weeks. Would not recommend to anyone.\"\"\"\n",
    "\n",
    "# Step 1: Define the prompt templates for each processing step\n",
    "sentiment_template = \"\"\"Analyze the sentiment of the following product review as positive, negative, or neutral.\n",
    "Provide your analysis in the format: \"SENTIMENT: [positive/negative/neutral]\"\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Your analysis:\n",
    "\"\"\"\n",
    "\n",
    "summary_template = \"\"\"Summarize the following product review into 3-5 key bullet points.\n",
    "Each bullet point should be concise and capture an important aspect mentioned in the review.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment: {sentiment}\n",
    "\n",
    "Key points:\n",
    "\"\"\"\n",
    "\n",
    "response_template = \"\"\"Write a helpful response to a customer based on their product review.\n",
    "If the sentiment is positive, thank them for their feedback. If negative, express understanding \n",
    "and suggest a solution or next steps. Personalize based on the specific points they mentioned.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment: {sentiment}\n",
    "Key points: {summary}\n",
    "\n",
    "Response to customer:\n",
    "\"\"\"\n",
    "\n",
    "# Create prompt templates for each step\n",
    "sentiment_prompt = PromptTemplate.from_template(sentiment_template)\n",
    "summary_prompt = PromptTemplate.from_template(summary_template)\n",
    "response_prompt = PromptTemplate.from_template(response_template)\n",
    "\n",
    "\n",
    "# PART 1: Traditional Chain Approach\n",
    "# Create individual LLMChains for each step\n",
    "sentiment_chain = LLMChain(\n",
    "    llm=llama_llm, \n",
    "    prompt=sentiment_prompt, \n",
    "    output_key=\"sentiment\"\n",
    ")\n",
    "\n",
    "summary_chain = LLMChain(\n",
    "    llm=llama_llm, \n",
    "    prompt=summary_prompt, \n",
    "    output_key=\"summary\"\n",
    ")\n",
    "\n",
    "response_chain = LLMChain(\n",
    "    llm=llama_llm, \n",
    "    prompt=response_prompt, \n",
    "    output_key=\"response\"\n",
    ")\n",
    "\n",
    "# Create a SequentialChain to connect all steps\n",
    "traditional_chain = SequentialChain(\n",
    "    chains=[sentiment_chain, summary_chain, response_chain],\n",
    "    input_variables=[\"review\"],\n",
    "    output_variables=[\"sentiment\", \"summary\", \"response\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# PART 2: LCEL Approach\n",
    "# Create individual chain components using the pipe operator (|)\n",
    "sentiment_chain_lcel = sentiment_prompt | llama_llm | StrOutputParser()\n",
    "summary_chain_lcel = summary_prompt | llama_llm | StrOutputParser()\n",
    "response_chain_lcel = response_prompt | llama_llm | StrOutputParser()\n",
    "\n",
    "# Connect the components using RunnablePassthrough.assign()\n",
    "lcel_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        sentiment=lambda x: sentiment_chain_lcel.invoke({\"review\": x[\"review\"]})\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        summary=lambda x: summary_chain_lcel.invoke({\n",
    "            \"review\": x[\"review\"], \n",
    "            \"sentiment\": x[\"sentiment\"]\n",
    "        })\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        response=lambda x: response_chain_lcel.invoke({\n",
    "            \"review\": x[\"review\"], \n",
    "            \"sentiment\": x[\"sentiment\"], \n",
    "            \"summary\": x[\"summary\"]\n",
    "        })\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Test both implementations\n",
    "def test_chains(review):\n",
    "    \"\"\"Test both chain implementations with the given review\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"TESTING WITH REVIEW:\\n{review[:100]}...\\n\")\n",
    "    \n",
    "    print(\"TRADITIONAL CHAIN RESULTS:\")\n",
    "    traditional_results = traditional_chain.invoke({\"review\": review})\n",
    "    print(f\"Sentiment: {traditional_results['sentiment']}\")\n",
    "    print(f\"Summary: {traditional_results['summary']}\")\n",
    "    print(f\"Response: {traditional_results['response']}\")\n",
    "    \n",
    "    print(\"\\nLCEL CHAIN RESULTS:\")\n",
    "    lcel_results = lcel_chain.invoke({\"review\": review})\n",
    "    print(f\"Sentiment: {lcel_results['sentiment']}\")\n",
    "    print(f\"Summary: {lcel_results['summary']}\")\n",
    "    print(f\"Response: {lcel_results['response']}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Run tests\n",
    "test_chains(positive_review)\n",
    "test_chains(negative_review)\n",
    "```\n",
    "</detail>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c662ab-7a68-4381-adb4-67d996594150",
   "metadata": {},
   "source": [
    "### Tools and Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b77efa-8296-4ba4-b7e3-5184ecd01945",
   "metadata": {},
   "source": [
    "##### **Tools**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87de29-7c73-48c6-bce6-0a369b7078ee",
   "metadata": {},
   "source": [
    "Tools extend an LLM's capabilities beyond just generating text. They allow the model to actually perform actions in the world or access external systems. This notebook shows the Python REPL tool, but there are many other tools:\n",
    "\n",
    "- Search tools: Connect to search engines, database queries, or vector stores.\n",
    "- API tools: Make calls to external web services.\n",
    "- Human-in-the-loop tools: Request human input for critical decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d56d8-fe33-4616-a3a1-e17ff48b0470",
   "metadata": {},
   "source": [
    "You can find a list of tools that LangChain supports at [https://python.langchain.com/docs/how_to/#tools](https://python.langchain.com/docs/how_to/#tools).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f21212-1c2a-431c-8fc3-c354cffbf132",
   "metadata": {},
   "source": [
    "Lets explore how to work with tools, using the `Python REPL` tool as an example. The `Python REPL` tool can run Python commands. These commands can either come from the user or the LLM can generate the commands. This tool is particularly useful for complex calculations. Instead of having the LLM generate the answer directly, using the LLM to generate code to calculate the answer is more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cd9edc15-2b0b-4538-8551-0ce15f3c70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981a42f-0b76-4c31-9dc4-8ec3166e8fab",
   "metadata": {},
   "source": [
    "The `@tool` decorator is a convenient way to define tools, but you can also use the Tool class directly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "80177b2d-bbdc-445f-bbd7-a71fd681d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PythonREPL instance\n",
    "# This provides an environment where Python code can be executed as strings\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "# Create a Tool using the Tool class\n",
    "# This wraps the Python REPL functionality as a tool that can be used by agents\n",
    "python_calculator = Tool(\n",
    "    # The name of the tool - this helps agents identify when to use this tool\n",
    "    name=\"Python Calculator\",\n",
    "    \n",
    "    # The function that will be called when the tool is used\n",
    "    # python_repl.run takes a string of Python code and executes it\n",
    "    func=python_repl.run,\n",
    "    \n",
    "    # A description of what the tool does and how to use it\n",
    "    # This helps the agent understand when and how to use this tool\n",
    "    description=\"Useful for when you need to perform calculations or execute Python code. Input should be valid Python code.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ef8e7-e28e-478d-9db3-ba30240e4d3e",
   "metadata": {},
   "source": [
    "Let's test this tool with a simple Python command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3dfb2741-088a-4882-9c15-b0403244be23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_calculator.invoke(\"a = 3; b = 1; print(a+b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26ca9b-5108-40e0-92b9-a4bddef9e976",
   "metadata": {},
   "source": [
    "We can also create custom tools using the `@tool` decorator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d0683-4af4-47a5-877e-c7f13fc33d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_weather(location: str):\n",
    "    \"\"\"Search for the current weather in the specified location.\"\"\"\n",
    "    # In a real application, this would call a weather API\n",
    "    return f\"The weather in {location} is currently sunny and 72F.\"\n",
    "\n",
    "search_weather(location=\"Delhi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd88eca-681c-4a40-bfa6-a2970acdfa23",
   "metadata": {},
   "source": [
    "##### **Toolkits**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c85145-4a8f-4168-afae-081d29e10ea8",
   "metadata": {},
   "source": [
    "Toolkits are collections of tools that are designed to be used together for specific tasks.\n",
    "\n",
    "Let's create a simple toolkit that contains multiple tools:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d2faef12-3a11-4832-b0b8-c53fde57b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toolkit (collection of tools)\n",
    "tools = [python_calculator, search_weather]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdbacb3-80f3-487d-ba99-66123c068350",
   "metadata": {},
   "source": [
    "A list of toolkits that Langchain supports is available at [https://python.langchain.com/docs/concepts/tools/#toolkits](https://python.langchain.com/docs/concepts/tools/#toolkits).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49108d5e-729d-46ef-b852-b9ae08b7d4de",
   "metadata": {},
   "source": [
    "##### **Agents**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44340b-3f03-4bf8-a29d-a9f662651307",
   "metadata": {},
   "source": [
    "By themselves, language models can't take actions; they just output text. A big use case for LangChain is creating agents. Agents are systems that leverage a large language model (LLM) as a reasoning engine to identify appropriate actions and determine the required inputs for those actions. The results of those actions are to be fed back into the agent. The agent then makes a determination whether more actions are needed, or if the task is complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362d8ab3-6441-46f2-835a-a2313f80a8ac",
   "metadata": {},
   "source": [
    "The modern approach to creating agents in LangChain uses the `create_react_agent` function and `AgentExecutor`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1822bf46-38a0-4d60-807c-66644ee6b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "# from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.tools import Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54f194-2a77-4176-a9c6-01f1c35d97c9",
   "metadata": {},
   "source": [
    "First, you will create a prompt for the agent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "df90bc1e-69b3-42a8-a885-a505e48962d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ReAct agent prompt template\n",
    "# The ReAct prompt needs to instruct the model to follow the thought-action-observation pattern\n",
    "prompt_template = \"\"\"You are an agent who has access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "The available tools are: {tool_names}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "```\n",
    "Thought: I need to figure out what to do\n",
    "Action: tool_name\n",
    "Action Input: the input to the tool\n",
    "```\n",
    "\n",
    "After you use a tool, the observation will be provided to you:\n",
    "```\n",
    "Observation: result of the tool\n",
    "```\n",
    "\n",
    "Then you should continue with the thought-action-observation cycle until you have enough information to respond to the user's request directly.\n",
    "When you have the final answer, respond in this format:\n",
    "```\n",
    "Thought: I know the answer\n",
    "Final Answer: the final answer to the original query\n",
    "```\n",
    "\n",
    "Remember, when using the Python Calculator tool, the input must be valid Python code.\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd297bd-d407-4309-b020-8644e5690a92",
   "metadata": {},
   "source": [
    "Now, you will create the agent and executor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9cbc6483-3f58-49c4-bb8e-a7675bf86283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = create_react_agent(\n",
    "    llm=llama_llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47789c-bb29-48fa-a6be-379968fdd865",
   "metadata": {},
   "source": [
    "The `create_react_agent` function creates an agent that follows the Reasoning + Acting (ReAct) framework. This framework was introduced in a [2023 paper](https://arxiv.org/abs/2210.03629) and has become one of the most effective approaches for LLM-based agents.\n",
    "\n",
    "**Key aspects of `create_react_agent`:**\n",
    "\n",
    "**Input Parameters**:\n",
    "\n",
    "- llm: The language model that powers the agent's reasoning. This is the \"brain\" that decides what to do.\n",
    "- tools: The list of tools the agent can use to interact with the world.\n",
    "- prompt: The instructions that guide the agent's behavior and explain the tools.\n",
    "\n",
    "\n",
    "**How ReAct Works**:\n",
    "The ReAct framework follows a specific cycle:\n",
    "\n",
    "- Reasoning: The agent thinks about the problem and plans its approach\n",
    "- Action: It selects a tool and formulates the input\n",
    "- Observation: It receives the result of the tool execution\n",
    "- Repeat: It reasons about the observation and decides the next step\n",
    "\n",
    "\n",
    "**Output Format Control**:\n",
    "The ReAct agent must produce output in a structured format that includes:\n",
    "\n",
    "- Thought: The agent's reasoning process\n",
    "- Action: The tool to use\n",
    "- Action Input: The input to the tool\n",
    "- Observation: The result of the tool execution\n",
    "- Final Answer: The final response when the agent has solved the problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "27fa04d7-f677-4c53-a6d4-d9d0ebd6b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7319d-492b-41d4-af86-82003c6d5064",
   "metadata": {},
   "source": [
    "The `AgentExecutor` is a crucial component that manages the execution flow of the agent. This component handles the orchestration between the agent's reasoning and the actual tool execution.\n",
    "\n",
    "**Key responsibilities of `AgentExecutor`:**\n",
    "\n",
    "**Execution Loop Management**:\n",
    "\n",
    "- Sends the initial query to the agent\n",
    "- Parses the agent's response to identify tool calls\n",
    "- Executes the specified tools with the provided inputs\n",
    "- Feeds tool results back to the agent\n",
    "- Continues this loop until the agent reaches a final answer\n",
    "\n",
    "**Input Parameters**:\n",
    "\n",
    "- agent: The agent object created with create_react_agent\n",
    "- tools: The same list of tools provided to the agent\n",
    "- verbose: When set to True, displays the entire thought process, which is extremely helpful for debugging\n",
    "\n",
    "**Error Handling**:\n",
    "\n",
    "- Catches and manages errors that occur during tool execution\n",
    "- Can be configured with handle_parsing_errors=True to recover from agent output format errors\n",
    "- Can implement retry logic for failed tool executions\n",
    "\n",
    "**Memory and State**:\n",
    "\n",
    "- Maintain the conversation state across multiple steps\n",
    "- Can configure with different types of memory for storing conversation history\n",
    "\n",
    "**Early Stopping**:\n",
    "\n",
    "- Can enforce maximum iterations to prevent infinite loops\n",
    "- Implements timeouts to handle tool executions that take too long\n",
    "\n",
    "Let's test the agent with a simple problem that requires only one tool:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "44f2ca28-af5a-41b7-8558-70fe0f6c4994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 28.268372242s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash-lite', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '28s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[156]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ask the agent a question that requires only calculation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the square root of 256?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    166\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    172\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    173\u001b[39m         inputs,\n\u001b[32m    174\u001b[39m         outputs,\n\u001b[32m    175\u001b[39m         return_only_outputs,\n\u001b[32m    176\u001b[39m     )\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\agents\\agent.py:1605\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1604\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1607\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1608\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1611\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1612\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1613\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1614\u001b[39m             next_step_output,\n\u001b[32m   1615\u001b[39m             intermediate_steps,\n\u001b[32m   1616\u001b[39m             run_manager=run_manager,\n\u001b[32m   1617\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\agents\\agent.py:1305\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1298\u001b[39m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1302\u001b[39m     run_manager: CallbackManagerForChainRun | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1303\u001b[39m ) -> AgentFinish | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m   1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m-> \u001b[39m\u001b[32m1305\u001b[39m         \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1306\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1310\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1314\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\agents\\agent.py:1332\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1329\u001b[39m     intermediate_steps = \u001b[38;5;28mself\u001b[39m._prepare_intermediate_steps(intermediate_steps)\n\u001b[32m   1331\u001b[39m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_action_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_classic\\agents\\agent.py:451\u001b[39m, in \u001b[36mRunnableAgent.plan\u001b[39m\u001b[34m(self, intermediate_steps, callbacks, **kwargs)\u001b[39m\n\u001b[32m    443\u001b[39m final_output: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_runnable:\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[32m    450\u001b[39m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunnable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\runnables\\base.py:3520\u001b[39m, in \u001b[36mRunnableSequence.stream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3513\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   3514\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream\u001b[39m(\n\u001b[32m   3515\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3518\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3519\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m3520\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\runnables\\base.py:3506\u001b[39m, in \u001b[36mRunnableSequence.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3499\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   3500\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\n\u001b[32m   3501\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3504\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3505\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m3506\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_stream_with_config(\n\u001b[32m   3507\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3508\u001b[39m         \u001b[38;5;28mself\u001b[39m._transform,\n\u001b[32m   3509\u001b[39m         patch_config(config, run_name=(config \u001b[38;5;129;01mor\u001b[39;00m {}).get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name),\n\u001b[32m   3510\u001b[39m         **kwargs,\n\u001b[32m   3511\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\runnables\\base.py:2320\u001b[39m, in \u001b[36mRunnable._transform_stream_with_config\u001b[39m\u001b[34m(self, inputs, transformer, config, run_type, **kwargs)\u001b[39m\n\u001b[32m   2318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2320\u001b[39m         chunk: Output = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2321\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m   2322\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\runnables\\base.py:3472\u001b[39m, in \u001b[36mRunnableSequence._transform\u001b[39m\u001b[34m(self, inputs, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   3469\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3470\u001b[39m         final_pipeline = step.transform(final_pipeline, config)\n\u001b[32m-> \u001b[39m\u001b[32m3472\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\runnables\\base.py:1542\u001b[39m, in \u001b[36mRunnable.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1539\u001b[39m final: Input\n\u001b[32m   1540\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1542\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43michunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The default implementation of transform is to buffer input and\u001b[39;49;00m\n\u001b[32m   1544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# then call stream.\u001b[39;49;00m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# It'll attempt to gather all input into a single chunk using\u001b[39;49;00m\n\u001b[32m   1546\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# the `+` operator.\u001b[39;49;00m\n\u001b[32m   1547\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If the input is not addable, then we'll assume that we can\u001b[39;49;00m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# only operate on the last chunk,\u001b[39;49;00m\n\u001b[32m   1549\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# and we'll iterate until we get to the last chunk.\u001b[39;49;00m\n\u001b[32m   1550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgot_first_val\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43michunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\runnables\\base.py:5764\u001b[39m, in \u001b[36mRunnableBindingBase.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5757\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\n\u001b[32m   5759\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5762\u001b[39m     **kwargs: Any,\n\u001b[32m   5763\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m5764\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.transform(\n\u001b[32m   5765\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5766\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5767\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5768\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\runnables\\base.py:1560\u001b[39m, in \u001b[36mRunnable.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m             final = ichunk\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(final, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\language_models\\chat_models.py:536\u001b[39m, in \u001b[36mBaseChatModel.stream\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    534\u001b[39m index = -\u001b[32m1\u001b[39m\n\u001b[32m    535\u001b[39m index_type = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_google_genai\\chat_models.py:3051\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._stream\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3049\u001b[39m index = -\u001b[32m1\u001b[39m\n\u001b[32m   3050\u001b[39m index_type = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3051\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_chat_result\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_response_to_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprev_usage_metadata\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\models.py:5407\u001b[39m, in \u001b[36mModels.generate_content_stream\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5401\u001b[39m function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m1\u001b[39m:\n\u001b[32m   5404\u001b[39m   \u001b[38;5;66;03m# First request gets a function call.\u001b[39;00m\n\u001b[32m   5405\u001b[39m   \u001b[38;5;66;03m# Then get function response parts.\u001b[39;00m\n\u001b[32m   5406\u001b[39m   \u001b[38;5;66;03m# Yield chunks only if there's no function response parts.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5407\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   5408\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunction_map\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   5409\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_extra_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend_chunk_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\models.py:4101\u001b[39m, in \u001b[36mModels._generate_content_stream\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4094\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4095\u001b[39m ):\n\u001b[32m   4096\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4097\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mAccessing the raw HTTP response is not supported in streaming\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   4098\u001b[39m       \u001b[33m'\u001b[39m\u001b[33m methods.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   4099\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m4101\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest_streamed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4102\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   4105\u001b[39m \u001b[43m  \u001b[49m\u001b[43mresponse_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4107\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\_api_client.py:1405\u001b[39m, in \u001b[36mBaseApiClient.request_streamed\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest_streamed\u001b[39m(\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1396\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1399\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1400\u001b[39m ) -> Generator[SdkHttpResponse, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m   1401\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1402\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1403\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1405\u001b[39m   session_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1406\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m session_response.segments():\n\u001b[32m   1407\u001b[39m     chunk_dump = json.dumps(chunk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1220\u001b[39m     retry_kwargs = retry_args(parameter_model.retry_options)\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:418\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    416\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:185\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tenacity\\__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\_api_client.py:1189\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1181\u001b[39m   httpx_request = \u001b[38;5;28mself\u001b[39m._httpx_client.build_request(\n\u001b[32m   1182\u001b[39m       method=http_request.method,\n\u001b[32m   1183\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1187\u001b[39m   )\n\u001b[32m   1188\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.send(httpx_request, stream=stream)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1190\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1191\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1192\u001b[39m   )\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m    134\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    148\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 28.268372242s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash-lite', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '28s'}]}}"
     ]
    }
   ],
   "source": [
    "# Ask the agent a question that requires only calculation\n",
    "result = agent_executor.invoke({\"input\": \"What is the square root of 256?\"})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0afae-a96c-4cc4-8e34-dad64ce17f51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, let's test the agent with different types of queries that would require it to use different tools from the toolkit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65a648-f370-4473-bf04-28425f440eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of different types of queries to test the agent\n",
    "queries = [\n",
    "    \"What's 345 * 789?\",\n",
    "    \"Calculate the square root of 144\",\n",
    "    \"What's the weather in Miami?\",\n",
    "    \"If it's sunny in Chicago, what would be a good outdoor activity?\",\n",
    "    \"Generate a list of prime numbers below 50 and calculate their sum\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = agent_executor.invoke({\"input\": query})\n",
    "    \n",
    "    print(f\"\\nFINAL ANSWER: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b665311-770d-4c16-9ed1-120ee93056c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As you can see, when faced with different queries, the ReAct agent follows a consistent yet adaptable thought process. \n",
    "\n",
    "For mathematical questions like \"Calculate the square root of 144,\" the agent recognizes the need for computation and selects the Python Calculator tool, writing code to calculate the answer. \n",
    "\n",
    "With weather-related queries like \"What's the weather in Miami?\", the agent immediately identifies the Weather Search tool as appropriate.\n",
    "\n",
    "At each step, the agent maintains a \"thought-action-observation\" cycle, explicitly reasoning about which tool to use, executing the chosen tool with appropriate input, observing the result, and continuing this process until the agent has all the information needed to provide a comprehensive final answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce3d806-1491-4a1f-adcc-2a500554ee90",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "#### **Creating Your First LangChain Agent with Basic Tools**\n",
    "\n",
    "In this exercise, you'll build a simple agent that can help users with basic tasks using two custom tools. This exercise is a perfect starting point for understanding how LangChain agents work.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create two simple tools: A calculator and a text formatter.\n",
    "2. Set up a basic agent that can use these tools.\n",
    "3. Test the agent with straightforward questions.\n",
    "\n",
    "**Starter code: provide your solution in the TODO parts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f8041-f874-4704-9531-76ddebf6ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# TODO: Create a simple calculator tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"A simple calculator that can add, subtract, multiply, or divide two numbers.\n",
    "    Input should be a mathematical expression like '2 + 2' or '15 / 3'.\"\"\"\n",
    "    try:\n",
    "        # HINT: Use Python's eval() function for simple calculations\n",
    "        result = eval(expression)\n",
    "        return f\"Result: {result}\"\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating: {str(e)}\"\n",
    "\n",
    "# TODO: Create a text formatting tool\n",
    "def format_text(text: str) -> str:\n",
    "    \"\"\"Format text to uppercase, lowercase, or title case.\n",
    "    Input should be in format: '[format_type]: [text]'\n",
    "    where format_type is 'uppercase', 'lowercase', or 'titlecase'.\"\"\"\n",
    "    try:\n",
    "        # HINT: Parse the input to get format type and text\n",
    "        # Your code here\n",
    "        if \":\" in text:\n",
    "            format_type, content = text.split(\":\", 1)\n",
    "            format_type = format_type.strip().lower()\n",
    "            content = content.strip()\n",
    "        else:\n",
    "            # If no colon, assume they want titlecase\n",
    "            return f\"Missing format. Example: titlecase: {text} -> {text.title()}\"\n",
    "            \n",
    "        if format_type == \"uppercase\":\n",
    "            return content.upper()\n",
    "        elif format_type == \"lowercase\":\n",
    "            return content.lower()\n",
    "        elif format_type == \"titlecase\":\n",
    "            return content.title()\n",
    "        else:\n",
    "            return f\"Unknown format {format_type}. Use: uppercase, lowercase, or titlecase\"\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        return f\"Error formatting text: {str(e)}\"\n",
    "\n",
    "# TODO: Create Tool objects for our functions\n",
    "# HINT: Use the Tool class to wrap the functions\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"calculator\",\n",
    "        func=calculator,\n",
    "        description=\"Useful for performing simple math calculations\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"format_text\",\n",
    "        func=format_text,\n",
    "        description=\"Useful for formatting text to uppercase, lowercase, or titlecase\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# TODO: Create a simple prompt template\n",
    "prompt_template = \"\"\"You are a helpful assistant who can use tools to help with simple tasks.\n",
    "You have access to these tools:\n",
    "\n",
    "{tools}\n",
    "The available tools are: {tool_names}\n",
    "\n",
    "Follow this format:\n",
    "\n",
    "Question: the user's question\n",
    "Thought: think about what to do\n",
    "Action: the tool to use, should be one of [{tool_names}]\n",
    "Action Input: the input to the tool\n",
    "Observation: the result from the tool\n",
    "Thought: I now know the final answer\n",
    "Final Answer: your final answer to the user's question\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Create the agent and executor\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "agent = create_react_agent(\n",
    "    llm=llama_llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Test with simple questions\n",
    "test_questions = [\n",
    "    \"What is 25 + 63?\", \n",
    "    \"Can you convert 'hello world' to uppercase?\",\n",
    "    \"Calculate 15 * 7\", \n",
    "    \"titlecase: langchain is awesome\",\n",
    "]\n",
    "\n",
    "# TODO: Run the tests\n",
    "for question in test_questions:\n",
    "    print(f\"\\n===== Testing: {question} =====\")\n",
    "    # Your code here\n",
    "    result = agent_executor.invoke({\"input\": question})\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e71509-7168-4c86-9b56-f9b92ef1a5cb",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hints</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a simple calculator tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"A simple calculator that can add, subtract, multiply, or divide two numbers.\n",
    "    Input should be a mathematical expression like '2 + 2' or '15 / 3'.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating: {str(e)}\"\n",
    "\n",
    "# Create a text formatting tool\n",
    "def format_text(text: str) -> str:\n",
    "    \"\"\"Format text to uppercase, lowercase, or title case.\n",
    "    Input should be in format: [format_type]: [text]\n",
    "    where format_type is uppercase, lowercase, or titlecase.\n",
    "    \n",
    "    Examples:\n",
    "    - uppercase: hello world -> HELLO WORLD\n",
    "    - lowercase: HELLO WORLD -> hello world \n",
    "    - titlecase: hello world -> Hello World\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle the case where the entire string is passed\n",
    "        if \":\" in text:\n",
    "            format_type, content = text.split(\":\", 1)\n",
    "            format_type = format_type.strip().lower()\n",
    "            content = content.strip()\n",
    "        else:\n",
    "            # If no colon, assume they want titlecase\n",
    "            return f\"Missing format. Example: titlecase: {text} -> {text.title()}\"\n",
    "            \n",
    "        if format_type == \"uppercase\":\n",
    "            return content.upper()\n",
    "        elif format_type == \"lowercase\":\n",
    "            return content.lower()\n",
    "        elif format_type == \"titlecase\":\n",
    "            return content.title()\n",
    "        else:\n",
    "            return f\"Unknown format {format_type}. Use: uppercase, lowercase, or titlecase\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error formatting text: {str(e)}\"\n",
    "\n",
    "# Create Tool objects for our functions\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"calculator\",\n",
    "        func=calculator,\n",
    "        description=\"Useful for performing simple math calculations\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"format_text\",\n",
    "        func=format_text,\n",
    "        description=\"Useful for formatting text to uppercase, lowercase, or titlecase\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create a simple prompt template\n",
    "# Note the added {tool_names} variable which was missing before\n",
    "prompt_template = \"\"\"You are a helpful assistant who can use tools to help with simple tasks.\n",
    "You have access to these tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "The available tools are: {tool_names}\n",
    "\n",
    "Follow this format:\n",
    "\n",
    "Question: the user's question\n",
    "Thought: think about what to do\n",
    "Action: the tool to use, should be one of [{tool_names}]\n",
    "Action Input: the input to the tool\n",
    "Observation: the result from the tool\n",
    "Thought: I now know the final answer\n",
    "Final Answer: your final answer to the user's question\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent and executor\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "agent = create_react_agent(\n",
    "    llm=llama_llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test with simple questions\n",
    "test_questions = [\n",
    "    \"What is 25 + 63?\", # The agent will be able to answer this question\n",
    "    \"Can you convert 'hello world' to uppercase?\", # The agent might be able to answer this question\n",
    "                                                    # However, it is not guaranteed due to incorrect input format\n",
    "    \"Calculate 15 * 7\", # The agent will be able to answer this question\n",
    "    \"titlecase: langchain is awesome\", # The agent will be able to answer this question\n",
    "]\n",
    "\n",
    "# Run the tests\n",
    "for question in test_questions:\n",
    "    print(f\"\\n===== Testing: {question} =====\")\n",
    "    result = agent_executor.invoke({\"input\": question})\n",
    "    print(f\"Final Answer: {result['output']}\")\n",
    "```\n",
    "\n",
    "</detail>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd14f9ae-ae9c-4271-b0e7-ff2f91721a07",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7f7cb-3e3a-4932-9342-e119b8d3e37c",
   "metadata": {},
   "source": [
    "[Hailey Quach](https://www.haileyq.com/)\n",
    "\n",
    "[Kang Wang](https://author.skills.network/instructors/kang_wang)\n",
    "\n",
    "[Faranak Heidari](https://author.skills.network/instructors/faranak_heidari) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae948cb-2f90-49bc-bec6-6fcdc5c7ef30",
   "metadata": {},
   "source": [
    "## Other contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ace3d1-c5d1-4ee5-8fc3-7decf3fd00ab",
   "metadata": {},
   "source": [
    "[Wojciech Fulmyk](https://author.skills.network/instructors/wojciech_fulmyk)\n",
    "\n",
    "[Ricky Shi](https://author.skills.network/instructors/ricky_shi) \n",
    "\n",
    "[Karan Goswami](https://author.skills.network/instructors/karan_goswami)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de978110-b27c-4b04-9ef2-d5982a3b8cff",
   "metadata": {},
   "source": [
    "<!-- ## Change log\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2025-03-06|1.1|Hailey Quach|Updated lab|\n",
    "|2025-03-28|1.2| P.Kravitz and Leah Hanson|Updated lab| \n",
    "|2025-03-28|1.3|Hailey Quach|Updated lab|\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4b530-9761-4472-ab98-ffff41dded0f",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "prev_pub_hash": "8cff305db4725499384d756dfb0c07b85259da62a36e8c6b237aba40a07f789d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
